{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVk6h8aAjTJv0RzLsr34a4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayendra-edu/jayendra-edu/blob/main/Logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is Logistic Regression, and how does it differ from Linear Regression.\n"
      ],
      "metadata": {
        "id": "_KyRVVdjLK1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression and Linear Regression are both statistical methods used for modeling relationships between variables, but they serve different purposes and are applied in different contexts.\n",
        "\n",
        "Linear Regression\n",
        "Purpose:\n",
        "\n",
        "Linear Regression is used to predict a continuous outcome variable (dependent variable) based on one or more predictor variables (independent variables).\n",
        "\n",
        "Model:\n",
        "\n",
        "The relationship between the dependent variable\n",
        "Y\n",
        "Y and the independent variables\n",
        "X\n",
        "X is modeled as a linear equation:\n",
        "\n",
        "Y\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        "+\n",
        "ϵ\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +ϵ\n",
        "where:\n",
        "\n",
        "Y\n",
        "Y is the dependent variable.\n",
        "\n",
        "X\n",
        "1\n",
        ",\n",
        "X\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "X\n",
        "n\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        "  are the independent variables.\n",
        "\n",
        "β\n",
        "0\n",
        ",\n",
        "β\n",
        "1\n",
        ",\n",
        "…\n",
        ",\n",
        "β\n",
        "n\n",
        "β\n",
        "0\n",
        "​\n",
        " ,β\n",
        "1\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  are the coefficients.\n",
        "\n",
        "ϵ\n",
        "ϵ is the error term.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "The relationship between the dependent and independent variables is linear.\n",
        "\n",
        "The residuals (errors) are normally distributed.\n",
        "\n",
        "Homoscedasticity (constant variance of residuals).\n",
        "\n",
        "No or little multicollinearity among independent variables.\n",
        "\n",
        "Output:\n",
        "\n",
        "The output is a continuous value.\n",
        "\n",
        "Logistic Regression\n",
        "Purpose:\n",
        "\n",
        "Logistic Regression is used to predict the probability of a binary outcome (e.g., yes/no, success/failure) based on one or more predictor variables.\n",
        "\n",
        "Model:\n",
        "\n",
        "The relationship between the dependent variable\n",
        "Y\n",
        "Y and the independent variables\n",
        "X\n",
        "X is modeled using the logistic function (sigmoid function):\n",
        "\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "e\n",
        "−\n",
        "(\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        ")\n",
        "P(Y=1)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "P(Y=1) is the probability of the dependent variable being 1 (or \"success\").\n",
        "\n",
        "X\n",
        "1\n",
        ",\n",
        "X\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "X\n",
        "n\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        "  are the independent variables.\n",
        "\n",
        "β\n",
        "0\n",
        ",\n",
        "β\n",
        "1\n",
        ",\n",
        "…\n",
        ",\n",
        "β\n",
        "n\n",
        "β\n",
        "0\n",
        "​\n",
        " ,β\n",
        "1\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  are the coefficients.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "The dependent variable is binary.\n",
        "\n",
        "The relationship between the logit of the dependent variable and the independent variables is linear.\n",
        "\n",
        "No multicollinearity among independent variables.\n",
        "\n",
        "Large sample size (as it relies on maximum likelihood estimation)."
      ],
      "metadata": {
        "id": "W9JuUNnBLK4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression."
      ],
      "metadata": {
        "id": "-6Lgac3xLK7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical equation of Logistic Regression is based on the logistic function (also called the sigmoid function), which maps any real-valued input to a probability value between 0 and 1. This makes it suitable for binary classification problems.\n",
        "\n",
        "Logistic Regression Equation\n",
        "The probability\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "P(Y=1) of the dependent variable\n",
        "Y\n",
        "Y being 1 (or \"success\") is modeled as:\n",
        "\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "e\n",
        "−\n",
        "(\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        ")\n",
        "P(Y=1)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "P(Y=1): The probability of the dependent variable\n",
        "Y\n",
        "Y being 1.\n",
        "\n",
        "X\n",
        "1\n",
        ",\n",
        "X\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "X\n",
        "n\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " : The independent variables (predictors).\n",
        "\n",
        "β\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        " : The intercept (bias term).\n",
        "\n",
        "β\n",
        "1\n",
        ",\n",
        "β\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "β\n",
        "n\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        " : The coefficients (weights) associated with each independent variable.\n",
        "\n",
        "e\n",
        "e: The base of the natural logarithm (approximately 2.71828).\n",
        "\n",
        "Sigmoid Function\n",
        "The equation above uses the sigmoid function, which is defined as:\n",
        "\n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "e\n",
        "−\n",
        "z\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Here,\n",
        "z\n",
        "z is the linear combination of the predictors and coefficients:\n",
        "\n",
        "z\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        "z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "The sigmoid function ensures that the output\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "P(Y=1) is always between 0 and 1, representing a probability.\n",
        "\n",
        "Logit (Log-Odds)\n",
        "Logistic Regression is often expressed in terms of the logit (log-odds), which is the natural logarithm of the odds of the event\n",
        "Y\n",
        "=\n",
        "1\n",
        "Y=1:\n",
        "\n",
        "logit\n",
        "(\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        ")\n",
        "=\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "1\n",
        "−\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        ")\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        "logit(P(Y=1))=ln(\n",
        "1−P(Y=1)\n",
        "P(Y=1)\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "This is the linear part of the model, where:\n",
        "\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "1\n",
        "−\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "1−P(Y=1)\n",
        "P(Y=1)\n",
        "​\n",
        " : The odds of\n",
        "Y\n",
        "=\n",
        "1\n",
        "Y=1.\n",
        "\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "1\n",
        "−\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        ")\n",
        "ln(\n",
        "1−P(Y=1)\n",
        "P(Y=1)\n",
        "​\n",
        " ): The log-odds (logit).\n",
        "\n",
        "Summary of the Equation\n",
        "Probability Form:\n",
        "\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "e\n",
        "−\n",
        "(\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        ")\n",
        "P(Y=1)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Logit Form:\n",
        "\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "1\n",
        "−\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        ")\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        "ln(\n",
        "1−P(Y=1)\n",
        "P(Y=1)\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "Interpretation\n",
        "The coefficients\n",
        "β\n",
        "1\n",
        ",\n",
        "β\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "β\n",
        "n\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  represent the change in the log-odds of the outcome for a one-unit increase in the corresponding predictor, holding all other predictors constant.\n",
        "\n",
        "The intercept\n",
        "β\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  represents the log-odds of the outcome when all predictors are zero.\n",
        "\n",
        "This equation forms the basis of Logistic Regression and is used to estimate the probability of a binary outcome based on the input features."
      ],
      "metadata": {
        "id": "2Utf5KkILK-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression."
      ],
      "metadata": {
        "id": "7ml6-ZIULLBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Sigmoid function is a key component of Logistic Regression because it provides a way to map the output of a linear equation to a probability value between 0 and 1. This is essential for binary classification problems, where the goal is to predict the probability of an event occurring (e.g., yes/no, success/failure). Here’s why the Sigmoid function is used:\n",
        "\n",
        "1. Outputs Probabilities (0 to 1)\n",
        "The Sigmoid function, defined as:\n",
        "\n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "e\n",
        "−\n",
        "z\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "ensures that the output\n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        "σ(z) is always between 0 and 1, regardless of the input\n",
        "z\n",
        "z.\n",
        "\n",
        "This is crucial because probabilities must lie in the range [0, 1], and the Sigmoid function naturally enforces this constraint.\n",
        "\n",
        "2. Handles Linear Combinations\n",
        "In Logistic Regression, the input\n",
        "z\n",
        "z is a linear combination of the predictors and coefficients:\n",
        "\n",
        "z\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        "z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "The Sigmoid function takes this linear combination and transforms it into a probability, allowing us to model the relationship between the predictors and the binary outcome.\n",
        "\n",
        "3. Interpretable as Log-Odds\n",
        "The Sigmoid function is invertible, and its inverse is the logit function:\n",
        "\n",
        "logit\n",
        "(\n",
        "p\n",
        ")\n",
        "=\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "p\n",
        "1\n",
        "−\n",
        "p\n",
        ")\n",
        "=\n",
        "z\n",
        "logit(p)=ln(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=z\n",
        "This means that the linear combination\n",
        "z\n",
        "z can be interpreted as the log-odds of the event occurring. For example:\n",
        "\n",
        "If\n",
        "z\n",
        "=\n",
        "0\n",
        "z=0, the odds are 1:1 (probability\n",
        "p\n",
        "=\n",
        "0.5\n",
        "p=0.5).\n",
        "\n",
        "If\n",
        "z\n",
        ">\n",
        "0\n",
        "z>0, the odds are greater than 1:1 (probability\n",
        "p\n",
        ">\n",
        "0.5\n",
        "p>0.5).\n",
        "\n",
        "If\n",
        "z\n",
        "<\n",
        "0\n",
        "z<0, the odds are less than 1:1 (probability\n",
        "p\n",
        "<\n",
        "0.5\n",
        "p<0.5).\n",
        "\n",
        "This makes the model interpretable and allows us to understand the impact of each predictor on the outcome.\n",
        "\n",
        "4. Smooth and Differentiable\n",
        "The Sigmoid function is smooth and differentiable, which is important for optimization algorithms like gradient descent.\n",
        "\n",
        "The derivative of the Sigmoid function is simple and can be expressed in terms of itself:\n",
        "\n",
        "σ\n",
        "′\n",
        "(\n",
        "z\n",
        ")\n",
        "=\n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        "⋅\n",
        "(\n",
        "1\n",
        "−\n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        ")\n",
        "σ\n",
        "′\n",
        " (z)=σ(z)⋅(1−σ(z))\n",
        "This property makes it computationally efficient to train Logistic Regression models.\n",
        "\n",
        "5. Thresholding for Classification\n",
        "Once the probability\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "P(Y=1) is computed using the Sigmoid function, a threshold (typically 0.5) can be applied to classify the outcome:\n",
        "\n",
        "If\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "≥\n",
        "0.5\n",
        "P(Y=1)≥0.5, predict\n",
        "Y\n",
        "=\n",
        "1\n",
        "Y=1.\n",
        "\n",
        "If\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "<\n",
        "0.5\n",
        "P(Y=1)<0.5, predict\n",
        "Y\n",
        "=\n",
        "0\n",
        "Y=0.\n",
        "\n",
        "This makes Logistic Regression a natural choice for binary classification tasks.\n",
        "\n",
        "6. Probabilistic Interpretation\n",
        "The Sigmoid function provides a probabilistic interpretation of the model. Instead of just predicting a class label, Logistic Regression estimates the probability of belonging to a class, which is useful in many real-world applications (e.g., risk scoring, medical diagnosis)."
      ],
      "metadata": {
        "id": "oK00VnlWLLEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression."
      ],
      "metadata": {
        "id": "PVryyMYlLLID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost function of Logistic Regression, also known as the log loss or logistic loss, is used to measure how well the model's predicted probabilities match the actual binary outcomes. Unlike Linear Regression, which uses the Mean Squared Error (MSE) as its cost function, Logistic Regression uses a cost function specifically designed for binary classification problems.\n",
        "\n",
        "Logistic Regression Cost Function\n",
        "The cost function for Logistic Regression is derived from the principle of Maximum Likelihood Estimation (MLE). It penalizes the model more heavily when it makes incorrect predictions with high confidence.\n",
        "\n",
        "For a binary classification problem with:\n",
        "\n",
        "y\n",
        "y: The actual binary label (0 or 1).\n",
        "\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        " : The predicted probability\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "P(Y=1) (output of the Sigmoid function).\n",
        "\n",
        "The cost function for a single training example is:\n",
        "\n",
        "Cost\n",
        "(\n",
        "y\n",
        ",\n",
        "y\n",
        "^\n",
        ")\n",
        "=\n",
        "{\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "y\n",
        "^\n",
        ")\n",
        "if\n",
        "y\n",
        "=\n",
        "1\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "^\n",
        ")\n",
        "if\n",
        "y\n",
        "=\n",
        "0\n",
        "Cost(y,\n",
        "y\n",
        "^\n",
        "​\n",
        " )={\n",
        "−log(\n",
        "y\n",
        "^\n",
        "​\n",
        " )\n",
        "−log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        " )\n",
        "​\n",
        "  \n",
        "if y=1\n",
        "if y=0\n",
        "​\n",
        "\n",
        "This can be combined into a single equation:\n",
        "\n",
        "Cost\n",
        "(\n",
        "y\n",
        ",\n",
        "y\n",
        "^\n",
        ")\n",
        "=\n",
        "−\n",
        "y\n",
        "log\n",
        "⁡\n",
        "(\n",
        "y\n",
        "^\n",
        ")\n",
        "−\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "^\n",
        ")\n",
        "Cost(y,\n",
        "y\n",
        "^\n",
        "​\n",
        " )=−ylog(\n",
        "y\n",
        "^\n",
        "​\n",
        " )−(1−y)log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        " )\n",
        "Intuition Behind the Cost Function\n",
        "When\n",
        "y\n",
        "=\n",
        "1\n",
        "y=1:\n",
        "\n",
        "The cost function becomes\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "y\n",
        "^\n",
        ")\n",
        "−log(\n",
        "y\n",
        "^\n",
        "​\n",
        " ).\n",
        "\n",
        "If the model predicts\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        "  close to 1 (correct prediction), the cost is close to 0.\n",
        "\n",
        "If the model predicts\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        "  close to 0 (incorrect prediction), the cost approaches infinity.\n",
        "\n",
        "When\n",
        "y\n",
        "=\n",
        "0\n",
        "y=0:\n",
        "\n",
        "The cost function becomes\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "^\n",
        ")\n",
        "−log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        " ).\n",
        "\n",
        "If the model predicts\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        "  close to 0 (correct prediction), the cost is close to 0.\n",
        "\n",
        "If the model predicts\n",
        "y\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        "  close to 1 (incorrect prediction), the cost approaches infinity.\n",
        "\n",
        "This behavior ensures that the model is heavily penalized for confident but incorrect predictions.\n",
        "\n",
        "Cost Function for the Entire Dataset\n",
        "For a dataset with\n",
        "m\n",
        "m training examples, the total cost function (also called the log loss) is the average of the individual costs over all examples:\n",
        "\n",
        "J\n",
        "(\n",
        "β\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "m\n",
        "∑\n",
        "i\n",
        "=\n",
        "1\n",
        "m\n",
        "[\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "y\n",
        "^\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "^\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "]\n",
        "J(β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "(i)\n",
        " )]\n",
        "Where:\n",
        "\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        "y\n",
        "(i)\n",
        " : The actual label of the\n",
        "i\n",
        "i-th training example.\n",
        "\n",
        "y\n",
        "^\n",
        "(\n",
        "i\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "(i)\n",
        " : The predicted probability\n",
        "P\n",
        "(\n",
        "Y\n",
        "=\n",
        "1\n",
        ")\n",
        "P(Y=1) for the\n",
        "i\n",
        "i-th training example.\n",
        "\n",
        "β\n",
        "β: The vector of model parameters (coefficients)."
      ],
      "metadata": {
        "id": "Drkswu6oMkka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed."
      ],
      "metadata": {
        "id": "hdCYxcD_MxpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. Overfitting occurs when a model learns the training data too well, capturing noise and irrelevant patterns, which harms its ability to generalize to unseen data. Regularization helps control the complexity of the model by discouraging large coefficients, leading to a simpler and more robust model.\n",
        "\n",
        "Types of Regularization in Logistic Regression\n",
        "There are two common types of regularization used in Logistic Regression:\n",
        "\n",
        "L1 Regularization (Lasso Regression):\n",
        "\n",
        "Adds a penalty proportional to the absolute value of the coefficients.\n",
        "\n",
        "The cost function with L1 regularization is:\n",
        "\n",
        "J\n",
        "(\n",
        "β\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "m\n",
        "∑\n",
        "i\n",
        "=\n",
        "1\n",
        "m\n",
        "[\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "y\n",
        "^\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "^\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "λ\n",
        "∑\n",
        "j\n",
        "=\n",
        "1\n",
        "n\n",
        "∣\n",
        "β\n",
        "j\n",
        "∣\n",
        "J(β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "(i)\n",
        " )]+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Effect: Encourages sparsity by driving some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        "L2 Regularization (Ridge Regression):\n",
        "\n",
        "Adds a penalty proportional to the square of the coefficients.\n",
        "\n",
        "The cost function with L2 regularization is:\n",
        "\n",
        "J\n",
        "(\n",
        "β\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "m\n",
        "∑\n",
        "i\n",
        "=\n",
        "1\n",
        "m\n",
        "[\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "y\n",
        "^\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "^\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "λ\n",
        "∑\n",
        "j\n",
        "=\n",
        "1\n",
        "n\n",
        "β\n",
        "j\n",
        "2\n",
        "J(β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "(i)\n",
        " )]+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Effect: Shrinks all coefficients toward zero but does not set them exactly to zero, reducing their impact.\n",
        "\n",
        "Elastic Net Regularization:\n",
        "\n",
        "Combines L1 and L2 regularization.\n",
        "\n",
        "The cost function with Elastic Net regularization is:\n",
        "\n",
        "J\n",
        "(\n",
        "β\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "m\n",
        "∑\n",
        "i\n",
        "=\n",
        "1\n",
        "m\n",
        "[\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "y\n",
        "^\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "y\n",
        "^\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "λ\n",
        "1\n",
        "∑\n",
        "j\n",
        "=\n",
        "1\n",
        "n\n",
        "∣\n",
        "β\n",
        "j\n",
        "∣\n",
        "+\n",
        "λ\n",
        "2\n",
        "∑\n",
        "j\n",
        "=\n",
        "1\n",
        "n\n",
        "β\n",
        "j\n",
        "2\n",
        "J(β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "  \n",
        "(i)\n",
        " )]+λ\n",
        "1\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Effect: Balances the benefits of both L1 and L2 regularization.\n",
        "\n",
        "Why is Regularization Needed?\n",
        "Prevents Overfitting:\n",
        "\n",
        "Without regularization, Logistic Regression may assign excessively large coefficients to some features, especially when there are many features or multicollinearity (high correlation between features). This leads to overfitting, where the model performs well on the training data but poorly on unseen data.\n",
        "\n",
        "Regularization penalizes large coefficients, encouraging the model to focus on the most important features and generalize better.\n",
        "\n",
        "Handles Multicollinearity:\n",
        "\n",
        "When features are highly correlated, the model may become unstable, and small changes in the data can lead to large changes in the coefficients.\n",
        "\n",
        "Regularization stabilizes the model by shrinking the coefficients of correlated features.\n",
        "\n",
        "Improves Interpretability:\n",
        "\n",
        "L1 regularization (Lasso) can reduce the number of features by setting some coefficients to zero, making the model simpler and easier to interpret.\n",
        "\n",
        "Controls Model Complexity:\n",
        "\n",
        "Regularization introduces a trade-off between fitting the training data well and keeping the model simple. This helps avoid overly complex models that capture noise in the data.\n",
        "\n",
        "Regularization Parameter (\n",
        "λ\n",
        "λ)\n",
        "The regularization parameter\n",
        "λ\n",
        "λ controls the strength of regularization:\n",
        "\n",
        "If\n",
        "λ\n",
        "=\n",
        "0\n",
        "λ=0, there is no regularization, and the model may overfit.\n",
        "\n",
        "If\n",
        "λ\n",
        "λ is too large, the model may underfit, as it overly penalizes the coefficients, leading to a simplistic model.\n",
        "\n",
        "The optimal value of\n",
        "λ\n",
        "λ is typically chosen using cross-validation.\n",
        "\n",
        "Summary\n",
        "Regularization in Logistic Regression is essential because:\n",
        "\n",
        "It prevents overfitting by penalizing large coefficients.\n",
        "\n",
        "It handles multicollinearity and improves model stability.\n",
        "\n",
        "It improves interpretability by simplifying the model (especially with L1 regularization).\n",
        "\n",
        "It controls model complexity, balancing the trade-off between bias and variance.\n",
        "\n",
        "By adding a regularization term to the cost function, Logistic Regression becomes more robust and generalizes better to unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "9UDFlWXoNFCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  the difference between Lasso, Ridge, and Elastic Net regressionC\n",
        "\n"
      ],
      "metadata": {
        "id": "SbJbL_XuNYFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso, Ridge, and Elastic Net regression are all regularization techniques used to prevent overfitting in regression models, including Logistic Regression. They differ in how they penalize the model's coefficients and the effects they have on the model. Here's a detailed comparison:\n",
        "\n",
        "1. Lasso Regression (L1 Regularization)\n",
        "Penalty Term:\n",
        "\n",
        "Adds a penalty proportional to the absolute value of the coefficients:\n",
        "\n",
        "Penalty\n",
        "=\n",
        "λ\n",
        "∑\n",
        "j\n",
        "=\n",
        "1\n",
        "n\n",
        "∣\n",
        "β\n",
        "j\n",
        "∣\n",
        "Penalty=λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "where\n",
        "λ\n",
        "λ is the regularization parameter and\n",
        "β\n",
        "j\n",
        "β\n",
        "j\n",
        "​\n",
        "  are the model coefficients.\n",
        "\n",
        "Effect on Coefficients:\n",
        "\n",
        "Encourages sparsity by driving some coefficients to exactly zero.\n",
        "\n",
        "Effectively performs feature selection by excluding irrelevant features from the model.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "When you have a large number of features and suspect that only a few are important.\n",
        "\n",
        "When interpretability is important, and you want to identify the most relevant features.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Produces simpler and more interpretable models.\n",
        "\n",
        "Automatically selects features by setting some coefficients to zero.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "May struggle with highly correlated features, as it tends to select only one and ignore the rest.\n",
        "\n",
        "2. Ridge Regression (L2 Regularization)\n",
        "Penalty Term:\n",
        "\n",
        "Adds a penalty proportional to the square of the coefficients:\n",
        "\n",
        "Penalty\n",
        "=\n",
        "λ\n",
        "∑\n",
        "j\n",
        "=\n",
        "1\n",
        "n\n",
        "β\n",
        "j\n",
        "2\n",
        "Penalty=λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Effect on Coefficients:\n",
        "\n",
        "Shrinks all coefficients toward zero but does not set them exactly to zero.\n",
        "\n",
        "Reduces the impact of all features, but retains all of them in the model.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "When you have multicollinearity (highly correlated features).\n",
        "\n",
        "When you want to reduce overfitting without eliminating features.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Handles multicollinearity well by distributing the effect among correlated features.\n",
        "\n",
        "Provides a more stable solution compared to Lasso.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Does not perform feature selection; all features remain in the model.\n",
        "\n",
        "May not be ideal when only a few features are relevant.\n",
        "\n",
        "3. Elastic Net Regression (Combination of L1 and L2 Regularization)\n",
        "Penalty Term:\n",
        "\n",
        "Combines both L1 and L2 penalties:\n",
        "\n",
        "Penalty\n",
        "=\n",
        "λ\n",
        "1\n",
        "∑\n",
        "j\n",
        "=\n",
        "1\n",
        "n\n",
        "∣\n",
        "β\n",
        "j\n",
        "∣\n",
        "+\n",
        "λ\n",
        "2\n",
        "∑\n",
        "j\n",
        "=\n",
        "1\n",
        "n\n",
        "β\n",
        "j\n",
        "2\n",
        "Penalty=λ\n",
        "1\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "where\n",
        "λ\n",
        "1\n",
        "λ\n",
        "1\n",
        "​\n",
        "  and\n",
        "λ\n",
        "2\n",
        "λ\n",
        "2\n",
        "​\n",
        "  control the strength of L1 and L2 regularization, respectively.\n",
        "\n",
        "Effect on Coefficients:\n",
        "\n",
        "Balances the effects of Lasso and Ridge:\n",
        "\n",
        "Encourages sparsity (like Lasso) but also handles multicollinearity (like Ridge).\n",
        "\n",
        "Can shrink some coefficients to zero while keeping others small but non-zero.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "When you have a large number of features, some of which are correlated.\n",
        "\n",
        "When you want to perform feature selection while also handling multicollinearity.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Combines the benefits of both Lasso and Ridge.\n",
        "\n",
        "More flexible and often performs better than Lasso or Ridge alone.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Requires tuning two hyperparameters (\n",
        "λ\n",
        "1\n",
        "λ\n",
        "1\n",
        "​\n",
        "  and\n",
        "λ\n",
        "2\n",
        "λ\n",
        "2\n",
        "​\n",
        " ), which can be computationally expensive.\n",
        "\n",
        "Key Differences Between Lasso, Ridge, and Elastic Net\n",
        "Aspect\tLasso (L1)\tRidge (L2)\tElastic Net\n",
        "Penalty Term\n",
        "λ\n",
        "∑\n",
        "λ∑\t\\beta_j\t)\n",
        "λ\n",
        "∑\n",
        "β\n",
        "j\n",
        "2\n",
        "λ∑β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "λ\n",
        "1\n",
        "∑\n",
        "λ\n",
        "1\n",
        "​\n",
        " ∑\t\\beta_j\t+ \\lambda_2 \\sum \\beta_j^2 )\n",
        "Effect on Coefficients\tSets some coefficients to zero\tShrinks all coefficients toward zero\tBalances sparsity and shrinkage\n",
        "Feature Selection\tYes\tNo\tYes\n",
        "Handles Multicollinearity\tNo (selects one feature)\tYes (distributes effect)\tYes\n",
        "Use Cases\tFew relevant features, interpretability\tMulticollinearity, general overfitting\tLarge datasets, correlated features\n",
        "Hyperparameters\n",
        "λ\n",
        "λ\n",
        "λ\n",
        "λ\n",
        "λ\n",
        "1\n",
        "λ\n",
        "1\n",
        "​\n",
        "  and\n",
        "λ\n",
        "2\n",
        "λ\n",
        "2\n",
        "​\n"
      ],
      "metadata": {
        "id": "-cTqgI84NYIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.When should we use Elastic Net instead of Lasso or Ridge."
      ],
      "metadata": {
        "id": "Jxolw5lJNYKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge, lasso and elastic net - Cross Validated - Stack Exchange\n",
        "Lasso does a sparse selection, while Ridge does not. · When you have highly-correlated variables, Ridge regression shrinks the two coefficients ...\n",
        "\n",
        "Any disadvantages of elastic net over lasso? - Cross Validated\n",
        "Why Lasso or ElasticNet perform better than Ridge when the ...\n",
        "regression - What is elastic net regularization, and how does it solve ...\n",
        "How to explain the differences between lasso, ridge, and elastic net ..Elastic Net is particularly useful when dealing with datasets that have highly correlated features, as it combines the strengths of both Lasso and Ridge regression. It performs variable selection like Lasso while also addressing multicollinearity issues, making it a versatile choice in many modeling scenarios. When to Use Elastic Net Instead of Lasso or Ridge\n",
        "\n",
        "Correlated Features:\n",
        "\n",
        "Elastic Net is ideal when features are highly correlated. It tends to keep or remove groups of correlated features together, unlike Lasso, which may randomly select one feature from a group.\n",
        "Feature Selection and Coefficient Shrinkage:\n",
        "\n",
        "It combines the feature selection capability of Lasso (which can set coefficients to zero) with the coefficient shrinkage of Ridge (which reduces the impact of features without eliminating them). This makes it effective for datasets where both feature selection and regularization are needed.\n",
        "Performance Issues with Lasso or Ridge:\n",
        "\n",
        "If you have tried Lasso and Ridge separately and found that neither provided satisfactory results, Elastic Net may yield better predictions by leveraging the strengths of both methods.\n",
        "High-Dimensional Data:\n",
        "\n",
        "In situations where the number of features exceeds the number of observations, Elastic Net can be more robust than Lasso or Ridge alone, as it can handle the complexity of the data more effectively.\n",
        "Model Complexity:\n",
        "\n",
        "If you want to maintain a balance between model complexity and interpretability, Elastic Net allows for a more nuanced approach by adjusting the mixing parameter (l1_ratio), which controls the balance between Lasso and Ridge penalties.\n",
        "Summary of Use Cases:\n",
        "\n",
        "Use Lasso when you want to perform feature selection and have a relatively small number of features.\n",
        "Use Ridge when you have many features and want to prevent overfitting without eliminating any features.\n",
        "Use Elastic Net when you have correlated features, need both feature selection and coefficient shrinkage, or when previous methods have not performed well.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k8nbxsO8NYMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is the impact of the regularization parameter (λ) in Logistic Regression."
      ],
      "metadata": {
        "id": "liRX7mu4NYPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In logistic regression, the regularization parameter (often denoted as ( \\lambda ) or sometimes ( \\alpha )) plays a crucial role in controlling the complexity of the model and preventing overfitting. Regularization is a technique used to impose a penalty on the size of the coefficients in the model, which can help improve generalization to unseen data. Here’s how the regularization parameter impacts logistic regression:\n",
        "\n",
        "1. Control Overfitting:\n",
        "High ( \\lambda ): A larger value of ( \\lambda ) increases the penalty on the coefficients, which can lead to simpler models with smaller coefficients. This can help prevent overfitting, especially in cases where the model is complex or the dataset is small.\n",
        "Low ( \\lambda ): A smaller value of ( \\lambda ) reduces the penalty, allowing the model to fit the training data more closely. This can lead to overfitting, where the model captures noise in the training data rather than the underlying pattern.\n",
        "2. Impact on Coefficients:\n",
        "L1 Regularization (Lasso): If ( \\lambda ) is used in L1 regularization, it can lead to some coefficients being exactly zero, effectively performing feature selection. This is useful when you suspect that many features are irrelevant.\n",
        "L2 Regularization (Ridge): In L2 regularization, ( \\lambda ) shrinks the coefficients but does not set them to zero. This can be beneficial when you want to retain all features but reduce their impact.\n",
        "3. Bias-Variance Tradeoff:\n",
        "Increasing ( \\lambda ): As ( \\lambda ) increases, the model becomes more biased (as it may underfit the training data), but the variance decreases (as it is less sensitive to fluctuations in the training data).\n",
        "Decreasing ( \\lambda ): Conversely, a smaller ( \\lambda ) can lead to lower bias (better fit to the training data) but higher variance (more sensitive to noise).\n",
        "4. Model Interpretability:\n",
        "A higher ( \\lambda ) can lead to a more interpretable model, as it simplifies the model by reducing the number of features or shrinking coefficients. This can make it easier to understand the influence of each feature on the outcome.\n",
        "5. Selection of ( \\lambda ):\n",
        "The choice of ( \\lambda ) is critical and is often determined through techniques such as cross-validation. This helps in finding a balance that minimizes the prediction error on unseen data.\n",
        "Summary:\n",
        "In summary, the regularization parameter ( \\lambda ) in logistic regression is essential for controlling model complexity, preventing overfitting, and managing the tradeoff between bias and variance. The appropriate choice of ( \\lambda ) can significantly impact the performance and interpretability of the logistic regression model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Bookmark message\n",
        "Copy message\n",
        "\n"
      ],
      "metadata": {
        "id": "JqHGgRqMNYRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression."
      ],
      "metadata": {
        "id": "mz3OUz8PNYVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In logistic regression, the regularization parameter (λ) plays a crucial role in controlling the complexity of the model and preventing overfitting. Regularization is a technique used to impose a penalty on the size of the coefficients in the model, which helps to ensure that the model generalizes well to unseen data. Here’s how the regularization parameter impacts logistic regression:\n",
        "\n",
        "Control Overfitting: A higher value of λ increases the penalty on the coefficients, which can help to reduce overfitting by discouraging the model from fitting the noise in the training data. This is particularly important when dealing with high-dimensional datasets where the risk of overfitting is greater.\n",
        "\n",
        "Coefficient Shrinkage: As λ increases, the coefficients of the logistic regression model are shrunk towards zero. This means that less important features may have their coefficients reduced significantly, effectively performing feature selection. In extreme cases, some coefficients may become exactly zero, leading to a simpler model.\n",
        "\n",
        "Bias-Variance Tradeoff: Increasing λ introduces bias into the model (as it constrains the coefficients), but it can reduce variance (the model's sensitivity to fluctuations in the training data). The goal is to find an optimal value of λ that balances bias and variance, leading to better performance on validation or test datasets.\n",
        "\n",
        "Types of Regularization: There are different types of regularization techniques that can be applied in logistic regression:\n",
        "\n",
        "L1 Regularization (Lasso): This adds the absolute value of the coefficients to the loss function. It can lead to sparse models where some coefficients are exactly zero.\n",
        "L2 Regularization (Ridge): This adds the square of the coefficients to the loss function. It tends to shrink coefficients but does not set them to zero, leading to a model that includes all features but with reduced impact from less important ones.\n",
        "Elastic Net: This combines both L1 and L2 regularization, allowing for a balance between feature selection and coefficient shrinkage.\n",
        "Model Interpretability: Regularization can also impact the interpretability of the model. A model with fewer non-zero coefficients (due to L1 regularization) may be easier to interpret, as it highlights the most important features.\n",
        "\n",
        "Hyperparameter Tuning: The choice of λ is often treated as a hyperparameter that needs to be tuned. Techniques such as cross-validation can be used to find the optimal value of λ that minimizes the loss function on validation data."
      ],
      "metadata": {
        "id": "lEshhTocRV1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression."
      ],
      "metadata": {
        "id": "Vh_Udd8JRafW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a widely used statistical method for binary classification problems. While it is a flexible and powerful technique, it is based on several key assumptions that should be considered when applying it to a dataset. Here are the main assumptions of logistic regression:\n",
        "\n",
        "Binary Outcome: Logistic regression is designed for binary outcome variables. The dependent variable should be categorical with two possible outcomes (e.g., success/failure, yes/no).\n",
        "\n",
        "Independence of Observations: The observations in the dataset should be independent of each other. This means that the outcome for one observation should not influence the outcome for another. Violations of this assumption can lead to biased estimates.\n",
        "\n",
        "Linearity of Logit: While logistic regression does not require a linear relationship between the independent variables and the dependent variable, it does assume that there is a linear relationship between the independent variables and the log odds (logit) of the dependent variable. This means that the logit transformation of the probability of the outcome should be a linear combination of the independent variables.\n",
        "\n",
        "No Multicollinearity: The independent variables should not be highly correlated with each other. Multicollinearity can inflate the variance of the coefficient estimates and make the model unstable. It can also make it difficult to determine the individual effect of each predictor.\n",
        "\n",
        "Large Sample Size: Logistic regression generally requires a sufficiently large sample size to provide reliable estimates. Small sample sizes can lead to overfitting and unreliable coefficient estimates. A common rule of thumb is to have at least 10 events (i.e., instances of the outcome) per predictor variable.\n",
        "\n",
        "Absence of Outliers: Logistic regression can be sensitive to outliers, which can disproportionately influence the model's estimates. It is important to check for and address outliers in the dataset.\n",
        "\n",
        "Homoscedasticity: While logistic regression does not assume homoscedasticity in the same way that linear regression does, it is still important that the variance of the errors is consistent across the range of predicted probabilities. This is less of a concern in logistic regression than in linear regression, but it is still worth considering.\n",
        "\n",
        "No Perfect Separation: Logistic regression assumes that there is no perfect separation between the classes. Perfect separation occurs when a predictor variable can perfectly predict the outcome variable, leading to infinite estimates for the coefficients. In such cases, alternative methods may be needed."
      ],
      "metadata": {
        "id": "dXKFewgZRaiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks."
      ],
      "metadata": {
        "id": "KEE5jJglRalL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it comes to classification tasks, there are several alternatives to logistic regression that can be used depending on the nature of the data, the complexity of the problem, and the specific requirements of the analysis. Here are some popular alternatives:\n",
        "\n",
        "Decision Trees:\n",
        "\n",
        "Decision trees are a non-parametric method that splits the data into subsets based on feature values. They are easy to interpret and visualize but can be prone to overfitting.\n",
        "Random Forest:\n",
        "\n",
        "Random forests are an ensemble method that builds multiple decision trees and combines their predictions. This approach helps to reduce overfitting and improve accuracy compared to a single decision tree.\n",
        "Support Vector Machines (SVM):\n",
        "\n",
        "SVMs are powerful classifiers that find the optimal hyperplane to separate different classes in the feature space. They can handle both linear and non-linear classification tasks using kernel functions.\n",
        "K-Nearest Neighbors (KNN):\n",
        "\n",
        "KNN is a simple, instance-based learning algorithm that classifies a data point based on the majority class of its k nearest neighbors in the feature space. It is easy to implement but can be computationally expensive for large datasets.\n",
        "Naive Bayes:\n",
        "\n",
        "Naive Bayes classifiers are based on Bayes' theorem and assume independence among predictors. They are particularly effective for text classification and can handle large datasets efficiently.\n",
        "Gradient Boosting Machines (GBM):\n",
        "\n",
        "Gradient boosting is an ensemble technique that builds models sequentially, where each new model corrects the errors of the previous ones. Variants include XGBoost, LightGBM, and CatBoost, which are known for their performance and speed.\n",
        "Neural Networks:\n",
        "\n",
        "Neural networks, particularly deep learning models, can capture complex patterns in data. They are highly flexible and can be used for various classification tasks, including image and text classification.\n",
        "Linear Discriminant Analysis (LDA):\n",
        "\n",
        "LDA is a statistical method that finds a linear combination of features that best separates two or more classes. It assumes that the predictors are normally distributed and have the same covariance matrix for each class.\n",
        "Quadratic Discriminant Analysis (QDA):\n",
        "\n",
        "QDA is similar to LDA but allows for different covariance matrices for each class. It is useful when the assumption of equal covariance is not valid.\n",
        "Ensemble Methods:\n",
        "\n",
        "Techniques like bagging (e.g., Bootstrap Aggregating) and stacking can combine multiple models to improve classification performance. These methods leverage the strengths of different algorithms.\n",
        "Logistic Regression with Regularization:\n",
        "\n",
        "While still a form of logistic regression, using regularization techniques (L1 or L2) can enhance its performance and robustness, especially in high-dimensional datasets.\n",
        "Multi-class Classification Algorithms:\n",
        "\n",
        "For problems with more than two classes, algorithms like One-vs-Rest (OvR) or One-vs-One (OvO) can be applied to extend binary classifiers to multi-class problems."
      ],
      "metadata": {
        "id": "-twSllHBRan2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What are Classification Evaluation Metrics."
      ],
      "metadata": {
        "id": "HSZzypJxRaqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification evaluation metrics are quantitative measures used to assess the performance of classification models. These metrics help determine how well a model is performing in terms of correctly predicting the classes of instances in a dataset. Here are some of the most commonly used classification evaluation metrics:\n",
        "\n",
        "Accuracy:\n",
        "\n",
        "Accuracy is the ratio of correctly predicted instances to the total instances in the dataset. It is calculated as: [ \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Instances}} ]\n",
        "While accuracy is a straightforward metric, it can be misleading, especially in imbalanced datasets.\n",
        "Precision:\n",
        "\n",
        "Precision (also known as positive predictive value) measures the proportion of true positive predictions among all positive predictions. It is calculated as: [ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} ]\n",
        "High precision indicates that a model has a low false positive rate.\n",
        "Recall (Sensitivity or True Positive Rate):\n",
        "\n",
        "Recall measures the proportion of true positive predictions among all actual positive instances. It is calculated as: [ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} ]\n",
        "High recall indicates that a model has a low false negative rate.\n",
        "F1 Score:\n",
        "\n",
        "The F1 score is the harmonic mean of precision and recall. It provides a balance between the two metrics and is particularly useful when dealing with imbalanced datasets. It is calculated as: [ \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} ]\n",
        "The F1 score ranges from 0 to 1, with 1 being the best possible score.\n",
        "ROC Curve (Receiver Operating Characteristic Curve):\n",
        "\n",
        "The ROC curve is a graphical representation of a classifier's performance across different threshold values. It plots the true positive rate (recall) against the false positive rate at various threshold settings.\n",
        "AUC (Area Under the ROC Curve):\n",
        "\n",
        "The AUC measures the area under the ROC curve. It provides a single value that summarizes the model's ability to discriminate between positive and negative classes. AUC values range from 0 to 1, with 0.5 indicating no discrimination (random guessing) and 1 indicating perfect discrimination.\n",
        "Confusion Matrix:\n",
        "\n",
        "A confusion matrix is a table that summarizes the performance of a classification model by showing the counts of true positives, true negatives, false positives, and false negatives. It provides a comprehensive view of how well the model is performing.\n",
        "Specificity (True Negative Rate):\n",
        "\n",
        "Specificity measures the proportion of true negative predictions among all actual negative instances. It is calculated as: [ \\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}} ]\n",
        "High specificity indicates that a model has a low false positive rate.\n",
        "Log Loss (Cross-Entropy Loss):\n",
        "\n",
        "Log loss measures the performance of a classification model where the prediction is a probability value between 0 and 1. It quantifies the difference between the predicted probabilities and the actual class labels. Lower log loss values indicate better model performance.\n",
        "Matthews Correlation Coefficient (MCC):\n",
        "\n",
        "The MCC is a measure of the quality of binary classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure. It is calculated as: [ \\text{MCC} = \\frac{(\\text{True Positives} \\times \\text{True Negatives}) - (\\text{False Positives} \\times \\text{False Negatives})}{\\sqrt{(\\text{True Positives} + \\text{False Positives})(\\text{True Positives} + \\text{False Negatives})(\\text{True Negatives} + \\text{False Positives})(\\text{True Negatives} + \\text{False Negatives})}} ]\n",
        "The MCC value ranges from -1 to 1, where 1 indicates perfect prediction, 0 indicates no better than random prediction, and -1 indicates total disagreement between prediction and observation."
      ],
      "metadata": {
        "id": "-kXPzUtRRau2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression."
      ],
      "metadata": {
        "id": "MYv3das5Rax0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class imbalance occurs when the distribution of classes in a dataset is not uniform, meaning that one class (the majority class) has significantly more instances than the other class (the minority class). This imbalance can have a substantial impact on the performance of logistic regression and other classification algorithms. Here are some key ways in which class imbalance affects logistic regression:\n",
        "\n",
        "Bias Toward the Majority Class:\n",
        "\n",
        "Logistic regression, like many classification algorithms, tends to be biased toward the majority class. When the model is trained on imbalanced data, it may learn to predict the majority class more often, leading to high accuracy but poor performance on the minority class.\n",
        "Misleading Accuracy:\n",
        "\n",
        "In imbalanced datasets, accuracy can be a misleading metric. For example, if 90% of the instances belong to the majority class, a model that predicts the majority class for all instances would achieve 90% accuracy, despite failing to identify any instances of the minority class. This can give a false sense of model performance.\n",
        "Low Recall for the Minority Class:\n",
        "\n",
        "The model may have low recall (sensitivity) for the minority class, meaning it fails to identify a significant number of positive instances. This is particularly problematic in applications where the minority class is of high importance, such as fraud detection or disease diagnosis.\n",
        "Poor Decision Boundary:\n",
        "\n",
        "The decision boundary learned by logistic regression may not be optimal in the presence of class imbalance. The model may be overly influenced by the majority class, leading to a decision boundary that does not adequately separate the minority class from the majority class.\n",
        "Increased False Negatives:\n",
        "\n",
        "The model may produce a higher number of false negatives for the minority class, which can be detrimental in scenarios where missing a positive instance has serious consequences (e.g., failing to detect a disease).\n",
        "Strategies to Address Class Imbalance\n",
        "To mitigate the effects of class imbalance on logistic regression, several strategies can be employed:\n",
        "\n",
        "Resampling Techniques:\n",
        "\n",
        "Oversampling: Increase the number of instances in the minority class by duplicating existing instances or generating synthetic instances (e.g., using SMOTE - Synthetic Minority Over-sampling Technique).\n",
        "Undersampling: Reduce the number of instances in the majority class to balance the dataset. This can lead to loss of information, so it should be done carefully.\n",
        "Class Weights:\n",
        "\n",
        "Assign higher weights to the minority class during model training. This can be done by using the class_weight parameter in logistic regression implementations (e.g., in scikit-learn). This approach penalizes misclassifications of the minority class more heavily, encouraging the model to pay more attention to it.\n",
        "Anomaly Detection Techniques:\n",
        "\n",
        "In cases where the minority class is extremely rare, treating the problem as an anomaly detection task may be more appropriate. This involves identifying instances that deviate significantly from the majority class.\n",
        "Ensemble Methods:\n",
        "\n",
        "Use ensemble techniques like Random Forests or Gradient Boosting, which can be more robust to class imbalance. These methods can combine predictions from multiple models, improving performance on the minority class.\n",
        "Evaluation Metrics:\n",
        "\n",
        "Use evaluation metrics that are more informative in the context of class imbalance, such as precision, recall, F1 score, and AUC-ROC, rather than relying solely on accuracy.\n",
        "Threshold Adjustment:\n",
        "\n",
        "Adjust the decision threshold for classifying instances. By default, logistic regression uses a threshold of 0.5, but this can be modified to favor the minority class, depending on the specific context and costs associated with false positives and false negatives."
      ],
      "metadata": {
        "id": "Wn0AkyCZRa0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What is Hyperparameter Tuning in Logistic Regression."
      ],
      "metadata": {
        "id": "qWjBgUKtRa3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning in logistic regression (and in machine learning in general) refers to the process of optimizing the hyperparameters of the model to improve its performance on a given task. Hyperparameters are the parameters that are not learned from the data during training but are set before the training process begins. They can significantly influence the model's performance, and finding the right values for them is crucial for achieving optimal results.\n",
        "\n",
        "Key Hyperparameters in Logistic Regression\n",
        "Regularization Strength (C):\n",
        "\n",
        "In logistic regression, regularization is used to prevent overfitting by penalizing large coefficients. The hyperparameter ( C ) (inverse of regularization strength) controls the amount of regularization applied. A smaller value of ( C ) indicates stronger regularization, while a larger value indicates weaker regularization. The choice of ( C ) can significantly affect the model's complexity and performance.\n",
        "Regularization Type:\n",
        "\n",
        "Logistic regression can use different types of regularization:\n",
        "L1 Regularization (Lasso): Encourages sparsity in the model by pushing some coefficients to zero.\n",
        "L2 Regularization (Ridge): Penalizes the square of the coefficients, leading to smaller but non-zero coefficients.\n",
        "The choice between L1 and L2 regularization can be considered a hyperparameter.\n",
        "Solver:\n",
        "\n",
        "The solver is the algorithm used to optimize the logistic regression model. Different solvers (e.g., 'liblinear', 'saga', 'lbfgs', 'newton-cg') have different characteristics and may perform better on different types of datasets. Choosing the right solver can impact convergence speed and model performance.\n",
        "Maximum Iterations:\n",
        "\n",
        "This hyperparameter specifies the maximum number of iterations for the optimization algorithm to converge. If the model does not converge within this limit, it may lead to suboptimal results.\n",
        "Tolerance:\n",
        "\n",
        "The tolerance hyperparameter determines the stopping criterion for the optimization algorithm. It specifies the threshold for convergence; if the change in the cost function is less than this value, the algorithm will stop.\n",
        "Hyperparameter Tuning Techniques\n",
        "To find the optimal hyperparameters for logistic regression, several techniques can be employed:\n",
        "\n",
        "Grid Search:\n",
        "\n",
        "Grid search involves specifying a range of values for each hyperparameter and exhaustively evaluating all possible combinations. This method can be computationally expensive but is straightforward and guarantees finding the best combination within the specified grid.\n",
        "Random Search:\n",
        "\n",
        "Random search randomly samples combinations of hyperparameters from specified distributions. It is often more efficient than grid search, especially when dealing with a large number of hyperparameters or when some hyperparameters have little effect on performance.\n",
        "Cross-Validation:\n",
        "\n",
        "Cross-validation is used in conjunction with grid or random search to evaluate the performance of different hyperparameter combinations. It involves splitting the dataset into training and validation sets multiple times to ensure that the model's performance is robust and not dependent on a specific train-test split.\n",
        "Bayesian Optimization:\n",
        "\n",
        "Bayesian optimization is a more advanced technique that builds a probabilistic model of the function mapping hyperparameters to model performance. It uses this model to make informed decisions about which hyperparameters to evaluate next, often leading to better results with fewer evaluations.\n",
        "Automated Hyperparameter Tuning Libraries:\n",
        "\n",
        "Libraries such as Optuna, Hyperopt, and Scikit-Optimize provide tools for automated hyperparameter tuning, making it easier to implement advanced tuning techniques."
      ],
      "metadata": {
        "id": "6WXw5yCsTbk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  What are different solvers in Logistic Regression? Which one should be used."
      ],
      "metadata": {
        "id": "gxKOIIzuTbn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In logistic regression, the solver is the algorithm used to optimize the model's parameters (coefficients). Different solvers have different characteristics, and the choice of solver can affect the convergence speed, performance, and suitability for specific types of datasets. Here are some common solvers used in logistic regression, particularly in libraries like Scikit-learn:\n",
        "\n",
        "Common Solvers for Logistic Regression\n",
        "liblinear:\n",
        "\n",
        "Type: Coordinate Descent\n",
        "Use Case: Suitable for small to medium-sized datasets.\n",
        "Characteristics:\n",
        "Supports L1 and L2 regularization.\n",
        "Works well for binary classification problems.\n",
        "Can handle large datasets but may be slower for very large datasets compared to other solvers.\n",
        "Recommendation: Good choice for smaller datasets or when L1 regularization is needed.\n",
        "newton-cg:\n",
        "\n",
        "Type: Newton's Method\n",
        "Use Case: Suitable for larger datasets.\n",
        "Characteristics:\n",
        "Uses the Newton-Raphson method to find the optimal parameters.\n",
        "Supports L2 regularization only.\n",
        "Generally faster than liblinear for larger datasets.\n",
        "Recommendation: A good choice for larger datasets where L2 regularization is preferred.\n",
        "lbfgs:\n",
        "\n",
        "Type: Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)\n",
        "Use Case: Suitable for large datasets.\n",
        "Characteristics:\n",
        "An optimization algorithm that approximates the BFGS algorithm.\n",
        "Supports L2 regularization only.\n",
        "Efficient for high-dimensional datasets.\n",
        "Recommendation: A widely used and effective solver for large datasets, especially when L2 regularization is desired.\n",
        "saga:\n",
        "\n",
        "Type: Stochastic Average Gradient\n",
        "Use Case: Suitable for large datasets and online learning.\n",
        "Characteristics:\n",
        "Can handle both L1 and L2 regularization.\n",
        "Works well with large datasets and is particularly efficient for sparse data.\n",
        "Supports mini-batch learning, making it suitable for online learning scenarios.\n",
        "Recommendation: A good choice for very large datasets or when L1 regularization is needed, especially with sparse features.\n",
        "sgd:\n",
        "\n",
        "Type: Stochastic Gradient Descent\n",
        "Use Case: Suitable for large datasets and online learning.\n",
        "Characteristics:\n",
        "A simple and efficient approach for large-scale learning.\n",
        "Can handle both L1 and L2 regularization.\n",
        "Requires careful tuning of learning rate and other hyperparameters.\n",
        "Recommendation: Useful for very large datasets or when implementing online learning, but requires more tuning compared to other solvers.\n",
        "Which Solver to Use?\n",
        "The choice of solver depends on several factors, including:\n",
        "\n",
        "Dataset Size:\n",
        "\n",
        "For small to medium-sized datasets, liblinear is often a good choice. For larger datasets, lbfgs, newton-cg, or saga are generally more efficient.\n",
        "Regularization Type:\n",
        "\n",
        "If you need L1 regularization, liblinear or saga are suitable. For L2 regularization, lbfgs, newton-cg, or saga can be used.\n",
        "Sparsity of Data:\n",
        "\n",
        "If the dataset is sparse (many zero values), saga is particularly effective.\n",
        "Convergence Speed:\n",
        "\n",
        "If you need faster convergence, lbfgs and saga are often preferred for larger datasets."
      ],
      "metadata": {
        "id": "ti0IHbHQTbsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15  How is Logistic Regression extended for multiclass classification."
      ],
      "metadata": {
        "id": "8Yldn4MCTbvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is inherently a binary classification algorithm, meaning it is designed to predict one of two possible outcomes. However, it can be extended to handle multiclass classification problems (where there are more than two classes) using several approaches. The two most common methods for extending logistic regression to multiclass classification are One-vs-Rest (OvR) and Softmax Regression (Multinomial Logistic Regression).\n",
        "\n",
        "1. One-vs-Rest (OvR) Approach\n",
        "The One-vs-Rest (OvR) strategy involves training a separate binary logistic regression model for each class. Here’s how it works:\n",
        "\n",
        "Model Training: For each class ( k ), a binary logistic regression model is trained to distinguish between instances of class ( k ) and instances of all other classes combined (the \"rest\"). If there are ( K ) classes, ( K ) separate models are trained.\n",
        "\n",
        "Prediction: When making predictions for a new instance, each of the ( K ) models outputs a probability score. The class with the highest probability score is selected as the predicted class for that instance.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple to implement and understand.\n",
        "Works well when classes are not too imbalanced.\n",
        "Disadvantages:\n",
        "\n",
        "Requires training multiple models, which can be computationally expensive.\n",
        "The final decision is based on the maximum probability, which may not always reflect the true class distribution.\n",
        "2. Softmax Regression (Multinomial Logistic Regression)\n",
        "Softmax regression, also known as multinomial logistic regression, is a direct extension of logistic regression for multiclass classification. Here’s how it works:\n",
        "\n",
        "Model Training: Instead of training separate binary classifiers, a single model is trained that predicts the probabilities of each class simultaneously. The model uses the softmax function to convert the raw output scores (logits) into probabilities.\n",
        "\n",
        "Softmax Function: The softmax function is defined as follows for ( K ) classes: [ P(y = k | \\mathbf{x}) = \\frac{e^{\\beta_k^T \\mathbf{x}}}{\\sum_{j=1}^{K} e^{\\beta_j^T \\mathbf{x}}} ] where ( \\beta_k ) is the coefficient vector for class ( k ), and ( \\mathbf{x} ) is the feature vector. The denominator sums the exponentials of the scores for all classes, ensuring that the predicted probabilities sum to 1.\n",
        "\n",
        "Prediction: The predicted class for a new instance is the one with the highest probability: [ \\hat{y} = \\arg\\max_k P(y = k | \\mathbf{x}) ]\n",
        "\n",
        "Advantages:\n",
        "\n",
        "More efficient than the OvR approach since it trains a single model.\n",
        "Provides a natural way to model the probabilities of multiple classes.\n",
        "Disadvantages:\n",
        "\n",
        "Assumes that the classes are mutually exclusive, which may not be suitable for all problems.\n",
        "Can be sensitive to class imbalances.\n",
        "Conclusion\n",
        "Both the One-vs-Rest and Softmax regression approaches allow logistic regression to be applied to multiclass classification problems. The choice between these methods depends on the specific characteristics of the dataset and the problem at hand.\n",
        "\n",
        "One-vs-Rest is often preferred for its simplicity and interpretability, especially in cases where the number of classes is small.\n",
        "Softmax regression is more efficient for larger numbers of classes and provides a more unified approach to multiclass classification."
      ],
      "metadata": {
        "id": "gcQVzFkETbyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What are the advantages and disadvantages of Logistic Regression.\n"
      ],
      "metadata": {
        "id": "0MX6tKxaTb1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a widely used statistical method for binary and multiclass classification tasks. It has several advantages and disadvantages that make it suitable for certain applications while limiting its effectiveness in others. Here’s a summary of the key advantages and disadvantages of logistic regression:\n",
        "\n",
        "Advantages of Logistic Regression\n",
        "Simplicity and Interpretability:\n",
        "\n",
        "Logistic regression is easy to understand and interpret. The coefficients of the model can be directly interpreted as the change in the log odds of the outcome for a one-unit change in the predictor variable.\n",
        "Efficiency:\n",
        "\n",
        "Logistic regression is computationally efficient and can be trained quickly, even on large datasets. This makes it suitable for real-time applications.\n",
        "Probabilistic Output:\n",
        "\n",
        "Logistic regression provides probabilities for class membership, which can be useful for decision-making processes. This probabilistic interpretation allows for better understanding of the model's confidence in its predictions.\n",
        "Works Well with Linearly Separable Data:\n",
        "\n",
        "Logistic regression performs well when the classes are linearly separable. It can effectively model the relationship between the independent variables and the binary outcome.\n",
        "Robust to Noise:\n",
        "\n",
        "Logistic regression can be relatively robust to noise in the data, especially when regularization techniques (like L1 or L2 regularization) are applied.\n",
        "Feature Selection:\n",
        "\n",
        "With L1 regularization (Lasso), logistic regression can perform feature selection by shrinking some coefficients to zero, effectively removing less important features from the model.\n",
        "Multiclass Extension:\n",
        "\n",
        "Logistic regression can be extended to handle multiclass classification problems using techniques like One-vs-Rest (OvR) or Softmax regression.\n",
        "Disadvantages of Logistic Regression\n",
        "Assumption of Linearity:\n",
        "\n",
        "Logistic regression assumes a linear relationship between the independent variables and the log odds of the dependent variable. If the true relationship is non-linear, logistic regression may not perform well unless transformations or polynomial terms are added.\n",
        "Sensitivity to Outliers:\n",
        "\n",
        "Logistic regression can be sensitive to outliers, which can disproportionately influence the model's coefficients and predictions.\n",
        "Limited to Linear Decision Boundaries:\n",
        "\n",
        "The model can only create linear decision boundaries (in the case of binary classification). For complex datasets with non-linear relationships, logistic regression may not capture the underlying patterns effectively.\n",
        "Multicollinearity:\n",
        "\n",
        "Logistic regression can be affected by multicollinearity (high correlation between independent variables), which can lead to unstable coefficient estimates and make it difficult to interpret the model.\n",
        "Requires Large Sample Sizes:\n",
        "\n",
        "Logistic regression generally requires a sufficiently large sample size to provide reliable estimates, especially when dealing with multiple predictors or when the outcome is rare.\n",
        "Imbalanced Data:\n",
        "\n",
        "Logistic regression can struggle with imbalanced datasets, where one class is significantly more frequent than the other. This can lead to biased predictions favoring the majority class.\n",
        "Binary Nature:\n",
        "\n",
        "While logistic regression can be extended to multiclass problems, it is fundamentally a binary classifier. In some cases, other algorithms may be more suitable for multiclass classification tasks."
      ],
      "metadata": {
        "id": "PRdiW6OQTb4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression."
      ],
      "metadata": {
        "id": "fIaLrX4VTb8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a versatile statistical method widely used for binary and multiclass classification tasks across various fields. Here are some common use cases of logistic regression:\n",
        "\n",
        "1. Medical Diagnosis\n",
        "Use Case: Predicting the presence or absence of a disease based on patient data (e.g., symptoms, lab results).\n",
        "Example: Classifying whether a patient has diabetes based on factors like age, BMI, blood pressure, and glucose levels.\n",
        "2. Credit Scoring\n",
        "Use Case: Assessing the likelihood of a borrower defaulting on a loan.\n",
        "Example: Using demographic and financial information (e.g., income, credit history, debt-to-income ratio) to predict whether a loan applicant is likely to default.\n",
        "3. Marketing and Customer Retention\n",
        "Use Case: Predicting customer behavior, such as whether a customer will respond to a marketing campaign or churn.\n",
        "Example: Classifying customers as likely to respond or not to a promotional offer based on their past purchasing behavior and demographics.\n",
        "4. Spam Detection\n",
        "Use Case: Classifying emails as spam or not spam based on their content and metadata.\n",
        "Example: Using features like the presence of certain keywords, sender reputation, and email structure to predict whether an email is spam.\n",
        "5. Social Media Analysis\n",
        "Use Case: Predicting user engagement or sentiment based on social media posts.\n",
        "Example: Classifying tweets as positive, negative, or neutral based on the text content and user interactions.\n",
        "6. Fraud Detection\n",
        "Use Case: Identifying fraudulent transactions in financial systems.\n",
        "Example: Using transaction data (e.g., amount, location, time) to predict whether a transaction is legitimate or fraudulent.\n",
        "7. Election Prediction\n",
        "Use Case: Predicting election outcomes based on polling data and demographic information.\n",
        "Example: Classifying voters' likelihood to vote for a particular candidate based on factors like age, education, and political affiliation.\n",
        "8. Customer Segmentation\n",
        "Use Case: Segmenting customers into different groups based on their likelihood to purchase certain products.\n",
        "Example: Classifying customers as high, medium, or low propensity to buy based on their browsing and purchasing history.\n",
        "9. Quality Control\n",
        "Use Case: Predicting whether a product will pass or fail quality control tests based on manufacturing data.\n",
        "Example: Using features like temperature, pressure, and material properties to classify products as acceptable or defective.\n",
        "10. Insurance Underwriting\n",
        "Use Case: Assessing risk and determining insurance premiums based on applicant data.\n",
        "Example: Classifying applicants as low, medium, or high risk based on factors like age, health history, and lifestyle choices."
      ],
      "metadata": {
        "id": "BGHc92TCTb_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression."
      ],
      "metadata": {
        "id": "k81jm8LnTcCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax regression and logistic regression are both statistical methods used for classification tasks, but they are designed for different types of problems and have distinct characteristics. Here’s a breakdown of the key differences between the two:\n",
        "\n",
        "1. Type of Classification\n",
        "Logistic Regression:\n",
        "\n",
        "Binary Classification: Logistic regression is primarily used for binary classification problems, where the outcome variable has two possible classes (e.g., 0 or 1, yes or no).\n",
        "Output: It predicts the probability of the positive class (e.g., class 1) using the logistic (sigmoid) function.\n",
        "Softmax Regression:\n",
        "\n",
        "Multiclass Classification: Softmax regression, also known as multinomial logistic regression, is used for multiclass classification problems, where the outcome variable can take on more than two classes (e.g., class 1, class 2, class 3, etc.).\n",
        "Output: It predicts the probabilities of each class using the softmax function, ensuring that the predicted probabilities sum to 1.\n",
        "2. Mathematical Formulation\n",
        "Logistic Regression:\n",
        "\n",
        "The probability of the positive class is given by: [ P(y = 1 | \\mathbf{x}) = \\frac{1}{1 + e^{-(\\beta^T \\mathbf{x})}} ]\n",
        "The probability of the negative class is simply: [ P(y = 0 | \\mathbf{x}) = 1 - P(y = 1 | \\mathbf{x}) ]\n",
        "Softmax Regression:\n",
        "\n",
        "The probability of class ( k ) is given by the softmax function: [ P(y = k | \\mathbf{x}) = \\frac{e^{\\beta_k^T \\mathbf{x}}}{\\sum_{j=1}^{K} e^{\\beta_j^T \\mathbf{x}}} ]\n",
        "Here, ( K ) is the total number of classes, and ( \\beta_k ) is the coefficient vector for class ( k ).\n",
        "3. Decision Boundary\n",
        "Logistic Regression:\n",
        "\n",
        "Logistic regression creates a linear decision boundary between the two classes. The model predicts the class based on whether the predicted probability exceeds a certain threshold (commonly 0.5).\n",
        "Softmax Regression:\n",
        "\n",
        "Softmax regression can create multiple linear decision boundaries, one for each class. The model predicts the class with the highest probability among all classes.\n",
        "4. Implementation Complexity\n",
        "Logistic Regression:\n",
        "\n",
        "Simpler to implement and interpret, as it involves fitting a single model for binary classification.\n",
        "Softmax Regression:\n",
        "\n",
        "More complex, as it requires fitting multiple parameters (one for each class) and involves more computational overhead, especially for large numbers of classes.\n",
        "5. Use Cases\n",
        "Logistic Regression:\n",
        "\n",
        "Commonly used in binary classification tasks such as medical diagnosis (e.g., disease presence/absence), spam detection, and credit scoring.\n",
        "Softmax Regression:\n",
        "\n",
        "Used in multiclass classification tasks such as image classification (e.g., identifying objects in images), text classification (e.g., sentiment analysis), and any scenario where there are more than two classes to predict."
      ],
      "metadata": {
        "id": "i7BYW3ldTcFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification."
      ],
      "metadata": {
        "id": "VoejjjaPTcI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing between One-vs-Rest (OvR) and Softmax regression for multiclass classification depends on several factors, including the nature of the data, the specific problem at hand, computational considerations, and the desired interpretability of the model. Here are some key points to consider when making this decision:\n",
        "\n",
        "1. Nature of the Problem\n",
        "OvR:\n",
        "\n",
        "Use Case: OvR is often preferred when the classes are not mutually exclusive or when you want to treat each class independently. It can be useful in scenarios where you may want to predict multiple classes for a single instance (multi-label classification).\n",
        "Interpretability: Each class has its own binary classifier, which can make it easier to interpret the contribution of each feature to the prediction of each class.\n",
        "Softmax:\n",
        "\n",
        "Use Case: Softmax regression is suitable when the classes are mutually exclusive, meaning that each instance belongs to only one class. It is ideal for problems where you want to predict a single class label from multiple classes (e.g., classifying an image as a cat, dog, or bird).\n",
        "Probabilistic Output: Softmax provides a probability distribution over all classes, which can be useful for understanding the model's confidence in its predictions.\n",
        "2. Computational Considerations\n",
        "OvR:\n",
        "\n",
        "Training Time: OvR requires training one binary classifier for each class. This can lead to longer training times, especially if the number of classes is large.\n",
        "Memory Usage: More memory may be required to store multiple models, which can be a consideration for very large datasets or high-dimensional feature spaces.\n",
        "Softmax:\n",
        "\n",
        "Single Model: Softmax regression trains a single model that predicts probabilities for all classes simultaneously, which can be more efficient in terms of both training time and memory usage.\n",
        "Optimization: The optimization process is typically more straightforward since it involves a single loss function for all classes.\n",
        "3. Performance and Accuracy\n",
        "OvR:\n",
        "\n",
        "Performance: In some cases, OvR can perform better, especially if the classes are imbalanced or if there are significant overlaps between classes. Each binary classifier can specialize in distinguishing its class from the rest.\n",
        "Error Propagation: Errors in one classifier do not directly affect the others, which can be beneficial in certain scenarios.\n",
        "Softmax:\n",
        "\n",
        "Performance: Softmax regression can provide better performance when the classes are well-separated and the model can learn the relationships between classes. It captures the interactions between classes, which can lead to improved accuracy.\n",
        "Class Dependencies: Since Softmax considers all classes simultaneously, it can better model the dependencies between classes.\n",
        "4. Implementation and Libraries\n",
        "OvR:\n",
        "\n",
        "Availability: Many machine learning libraries (e.g., Scikit-learn) provide built-in support for OvR, making it easy to implement.\n",
        "Flexibility: OvR can be applied to any binary classifier, allowing for flexibility in choosing the underlying model.\n",
        "Softmax:\n",
        "\n",
        "Built-in Support: Softmax regression is also widely supported in machine learning libraries, particularly in frameworks designed for deep learning (e.g., TensorFlow, PyTorch).\n",
        "Unified Approach: Softmax provides a unified approach for multiclass classification, which can simplify the modeling process.\n",
        "Conclusion\n",
        "In summary, the choice between One-vs-Rest (OvR) and Softmax regression for multiclass classification depends on the specific characteristics of the problem, the nature of the data, and the computational resources available.\n",
        "\n",
        "Choose OvR if:\n",
        "\n",
        "The classes are not mutually exclusive or if you want to treat each class independently.\n",
        "You prefer interpretability and want to analyze the contribution of features to each class separately.\n",
        "You are dealing with imbalanced classes or overlapping classes.\n",
        "Choose Softmax if:\n",
        "\n",
        "The classes are mutually exclusive, and you want to predict a single class label.\n",
        "You want a probabilistic output that reflects the model's confidence across all classes.\n",
        "You are looking for a more efficient training process with a single model."
      ],
      "metadata": {
        "id": "zM8D6z2eTcMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?"
      ],
      "metadata": {
        "id": "ToTOB2TKTcPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting coefficients in logistic regression is crucial for understanding the relationship between the independent variables (features) and the dependent variable (outcome). Unlike linear regression, where coefficients represent the change in the dependent variable for a one-unit change in the independent variable, logistic regression coefficients represent the change in the log odds of the outcome for a one-unit change in the predictor variable. Here’s how to interpret these coefficients:\n",
        "\n",
        "1. Log Odds Interpretation\n",
        "Coefficient (β): In logistic regression, each coefficient (β) associated with an independent variable indicates the change in the log odds of the outcome for a one-unit increase in that variable, holding all other variables constant.\n",
        "\n",
        "[ \\text{Log Odds} = \\log\\left(\\frac{P(y=1 | X)}{P(y=0 | X)}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n ]\n",
        "\n",
        "Positive Coefficient: A positive coefficient indicates that as the independent variable increases, the log odds of the outcome being 1 (e.g., success, presence of a condition) also increase. This means that the probability of the outcome occurring increases.\n",
        "\n",
        "Negative Coefficient: A negative coefficient indicates that as the independent variable increases, the log odds of the outcome being 1 decrease, meaning the probability of the outcome occurring decreases.\n",
        "\n",
        "2. Odds Ratio Interpretation\n",
        "To make the interpretation more intuitive, we can convert the coefficients into odds ratios:\n",
        "\n",
        "Odds Ratio (OR): The odds ratio is calculated as ( e^{\\beta} ). It represents the multiplicative change in the odds of the outcome for a one-unit increase in the predictor variable.\n",
        "\n",
        "[ \\text{Odds Ratio} = e^{\\beta} ]\n",
        "\n",
        "Interpreting Odds Ratios:\n",
        "\n",
        "OR > 1: Indicates that for a one-unit increase in the predictor, the odds of the outcome occurring increase. For example, an OR of 1.5 means that the odds are 50% higher for each one-unit increase in the predictor.\n",
        "OR < 1: Indicates that for a one-unit increase in the predictor, the odds of the outcome occurring decrease. For example, an OR of 0.7 means that the odds are 30% lower for each one-unit increase in the predictor.\n",
        "OR = 1: Indicates no effect; the predictor does not influence the odds of the outcome.\n",
        "3. Example Interpretation\n",
        "Suppose we have a logistic regression model predicting whether a patient has a disease (1 = yes, 0 = no) based on age and cholesterol level, and we obtain the following coefficients:\n",
        "\n",
        "Intercept (β₀): -3.0\n",
        "Age (β₁): 0.05\n",
        "Cholesterol (β₂): 0.02\n",
        "Interpretation:\n",
        "\n",
        "Intercept: The log odds of having the disease when both age and cholesterol are zero is -3.0. This is not practically interpretable since age and cholesterol cannot be zero in real scenarios.\n",
        "\n",
        "Age: For each additional year of age, the log odds of having the disease increase by 0.05. The odds ratio is ( e^{0.05} \\approx 1.051 ), meaning that for each additional year of age, the odds of having the disease increase by approximately 5.1%.\n",
        "\n",
        "Cholesterol: For each one-unit increase in cholesterol level, the log odds of having the disease increase by 0.02. The odds ratio is ( e^{0.02} \\approx 1.020 ), meaning that for each one-unit increase in cholesterol, the odds of having the disease increase by approximately 2%.\n",
        "\n",
        "4. Considerations\n",
        "Statistical Significance: It’s important to assess the statistical significance of the coefficients (using p-values) to determine whether the relationships observed are meaningful.\n",
        "\n",
        "Multicollinearity: If independent variables are highly correlated, it can affect the stability and interpretability of the coefficients.\n",
        "\n",
        "Non-linearity: Logistic regression assumes a linear relationship between the log odds and the independent variables. If this assumption is violated, the interpretation may not hold."
      ],
      "metadata": {
        "id": "HucvcRFVVJNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M4duHdUEV2Ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "xjna-n2rV2Ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracyC"
      ],
      "metadata": {
        "id": "eU319DfmV2Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfbcQ8W6WJAO",
        "outputId": "35fa6648-1943-48f9-f657-59295637d30e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Below is a Python program that demonstrates how to load a dataset, split it into training and testing sets, apply logistic regression, and print the model's accuracy. For this example, we'll use the popular Iris dataset, which is available in the sklearn library. The Iris dataset is a classic dataset for classification tasks."
      ],
      "metadata": {
        "id": "EbPG9ky4V2Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (species)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfNqoDNYWaVn",
        "outputId": "b30296d8-0e44-46fb-86f6-daa5a3830e16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Code\n",
        "Import Libraries: We import the necessary libraries, including pandas, sklearn.datasets, sklearn.model_selection, sklearn.linear_model, and sklearn.metrics.\n",
        "\n",
        "Load the Dataset: We load the Iris dataset using load_iris(), which provides both the features (X) and the target variable (y).\n",
        "\n",
        "Split the Dataset: We use train_test_split() to split the dataset into training and testing sets. In this example, 20% of the data is reserved for testing, and we set a random seed for reproducibility.\n",
        "\n",
        "Create the Model: We create an instance of the LogisticRegression model. The max_iter parameter is set to 200 to ensure convergence.\n",
        "\n",
        "Fit the Model: We fit the model to the training data using the fit() method.\n",
        "\n",
        "Make Predictions: We use the trained model to make predictions on the test data with the predict() method.\n",
        "\n",
        "Calculate Accuracy: We calculate the accuracy of the model using accuracy_score() by comparing the predicted labels with the true labels.\n",
        "\n",
        "Print Accuracy: Finally, we print the model's accuracy."
      ],
      "metadata": {
        "id": "mdXrJqxyV2Y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2, Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracyC"
      ],
      "metadata": {
        "id": "7AHRbDxXV2bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that demonstrates how to apply L1 regularization (Lasso) using logistic regression on a dataset. We'll use the Iris dataset again for this example, but you can replace it with any dataset of your choice"
      ],
      "metadata": {
        "id": "NtR6jiozV2eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w32GvdiWspX",
        "outputId": "0fa84ec0-47f8-4b9b-95a9-46f959422f0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (species)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with L1 Regularization: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsHAAQ0FWylm",
        "outputId": "2cd92e15-593d-4e8e-9dd6-84208039be72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Code\n",
        "Import Libraries: We import the necessary libraries, including pandas, sklearn.datasets, sklearn.model_selection, sklearn.linear_model, and sklearn.metrics.\n",
        "\n",
        "Load the Dataset: We load the Iris dataset using load_iris(), which provides both the features (X) and the target variable (y).\n",
        "\n",
        "Split the Dataset: We use train_test_split() to split the dataset into training and testing sets. In this example, 20% of the data is reserved for testing, and we set a random seed for reproducibility.\n",
        "\n",
        "Create the Model: We create an instance of the LogisticRegression model. The max_iter parameter is set to 200 to ensure convergence.\n",
        "\n",
        "Fit the Model: We fit the model to the training data using the fit() method.\n",
        "\n",
        "Make Predictions: We use the trained model to make predictions on the test data with the predict() method.\n",
        "\n",
        "Calculate Accuracy: We calculate the accuracy of the model using accuracy_score() by comparing the predicted labels with the true labels.\n",
        "\n",
        "Print Accuracy: Finally, we print the model's accuracy."
      ],
      "metadata": {
        "id": "z1XsNcvwV2jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficientsC"
      ],
      "metadata": {
        "id": "7TmIysCpV2md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that demonstrates how to train a logistic regression model with L2 regularization (Ridge) using the LogisticRegression class from the sklearn library. The program will load the Iris dataset, split it into training and testing sets, fit the model, and then print the model's accuracy and coefficients."
      ],
      "metadata": {
        "id": "v3po30VVV2pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeleySQzXSWm",
        "outputId": "94a34631-4ac9-48ae-f97d-a6413672f35f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (species)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with L2 Regularization: {accuracy:.2f}')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn4Vca63XX_X",
        "outputId": "205c41fa-753d-4f30-c1a3-d1b46ce9d703"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization: 1.00\n",
            "Model Coefficients:\n",
            "[[-0.39340204  0.96258576 -2.37510761 -0.99874603]\n",
            " [ 0.50840364 -0.25486503 -0.21301366 -0.77575487]\n",
            " [-0.1150016  -0.70772072  2.58812127  1.77450091]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Code\n",
        "Import Libraries: We import the necessary libraries, including pandas, sklearn.datasets, sklearn.model_selection, sklearn.linear_model, and sklearn.metrics.\n",
        "\n",
        "Load the Dataset: We load the Iris dataset using load_iris(), which provides both the features (X) and the target variable (y).\n",
        "\n",
        "Split the Dataset: We use train_test_split() to split the dataset into training and testing sets. In this example, 20% of the data is reserved for testing, and we set a random seed for reproducibility.\n",
        "\n",
        "Create the Model: We create an instance of the LogisticRegression model with L2 regularization by setting penalty='l2'. The solver='lbfgs' is specified, which is suitable for L2 regularization. The max_iter parameter is set to 200 to ensure convergence.\n",
        "\n",
        "Fit the Model: We fit the model to the training data using the fit() method.\n",
        "\n",
        "Make Predictions: We use the trained model to make predictions on the test data with the predict() method.\n",
        "\n",
        "Calculate Accuracy: We calculate the accuracy of the model using accuracy_score() by comparing the predicted labels with the true labels.\n",
        "\n",
        "Print Accuracy: Finally, we print the model's accuracy and the coefficients of the model."
      ],
      "metadata": {
        "id": "aATSDj0LV2vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')C"
      ],
      "metadata": {
        "id": "hlVvW_4yV2yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a logistic regression model with Elastic Net regularization using the LogisticRegression class from the sklearn library, you need to specify penalty='elasticnet'. Elastic Net combines both L1 and L2 regularization, which can be useful for feature selection and handling multicollinearity.\n",
        "\n",
        "However, it's important to note that as of my last knowledge update, the elasticnet penalty requires the solver to be set to saga, which supports Elastic Net regularization. Below is a Python program that demonstrates how to implement this using the Iris dataset."
      ],
      "metadata": {
        "id": "HBO9_K0bV21I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (species)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with Elastic Net Regularization: {accuracy:.2f}')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wEDrATpXxRf",
        "outputId": "f17cf9e1-2145-4fae-f1a6-06cd7db379cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.38755932  1.77014188 -2.42311237 -0.70536972]\n",
            " [ 0.0789824   0.          0.         -0.58316319]\n",
            " [-1.25864292 -1.53121212  2.5958249   2.07923119]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'C\n",
        "\"C Write a Python prog"
      ],
      "metadata": {
        "id": "VT6v9Wx9V269"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that demonstrates how to train a logistic regression model for multiclass classification using the One-vs-Rest (OvR) strategy. We will use the Iris dataset for this example, which contains three classes of iris flowers."
      ],
      "metadata": {
        "id": "5_ZJ0ce1YK7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (species)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with One-vs-Rest (OvR) strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with One-vs-Rest (OvR): {accuracy:.2f}')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69HcY-L8YaJv",
        "outputId": "900df04f-2ae3-43a9-ec01-a3a33674c2cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-Rest (OvR): 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracyC"
      ],
      "metadata": {
        "id": "mwju2nZMYZVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that demonstrates how to evaluate a logistic regression model using Stratified K-Fold Cross-Validation. This method ensures that each fold of the cross-validation process has the same proportion of classes as the entire dataset, which is particularly useful for imbalanced datasets.\n",
        "\n",
        "In this example, we will use the Iris dataset for classification."
      ],
      "metadata": {
        "id": "5xo--u8AYK-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (species)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Set up Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "accuracies = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print the average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(f'Average Accuracy with Stratified K-Fold Cross-Validation: {average_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_Pm-sDyYrxP",
        "outputId": "ebc4940c-da2b-45c5-e3d1-a63c360e21de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy with Stratified K-Fold Cross-Validation: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "_xSWExAAYLBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that demonstrates how to use GridSearchCV from the sklearn library to tune the hyperparameters (specifically C and penalty) of a logistic regression model. We will use the Iris dataset for this example."
      ],
      "metadata": {
        "id": "MR4seLTEYLE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (species)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2']  # Regularization type\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "\n",
        "# Fit GridSearchCV on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Make predictions on the test data using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Best Cross-Validation Accuracy: {best_score:.2f}')\n",
        "print(f'Test Set Accuracy: {test_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OwlCQyAZF13",
        "outputId": "5a79a0d2-8f55-4944-bc72-f0241033a853"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best Cross-Validation Accuracy: 0.97\n",
            "Test Set Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.85833333        nan 0.93333333        nan 0.96666667\n",
            "        nan 0.94166667        nan 0.95      ]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy"
      ],
      "metadata": {
        "id": "-QDhYGtaYLHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that demonstrates how to load a dataset from a CSV file, apply logistic regression, and evaluate its accuracy. For this example, I'll assume that the CSV file contains a binary classification problem. You can replace the dataset with your own CSV file."
      ],
      "metadata": {
        "id": "cDgQb_GpYLJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U_aEdLUacHF",
        "outputId": "f2dd7b50-140e-4f56-8aaf-9eaf29ee2e9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from a CSV file\n",
        "# Replace 'your_dataset.csv' with the path to your CSV file\n",
        "data = pd.read_csv('your_dataset.csv')  # Intentional error: file may not exist\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Assume the last column is the target variable and the rest are features\n",
        "X = data.iloc[:, :-1]  # Features (all columns except the last)\n",
        "y = data.iloc[:, -1]   # Target variable (last column)\n",
        "\n",
        "# Introduce an intentional error: assume the target variable has non-numeric values\n",
        "# Uncomment the next line to simulate this error\n",
        "# y = ['class1', 'class2', 'class1', 'class2', 'class1']  # Non-numeric target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)  # This may raise an error if y is non-numeric\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lWtR9wN1aavo",
        "outputId": "d4c155c3-d7d7-4edc-a007-b713ce4d73b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b897067ba02c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the dataset from a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Replace 'your_dataset.csv' with the path to your CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Intentional error: file may not exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Display the first few rows of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracyM"
      ],
      "metadata": {
        "id": "X6TcYvfOYLMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tune hyperparameters in Logistic Regression using RandomizedSearchCV, you can follow these general steps:\n",
        "\n",
        "Import the necessary libraries.\n",
        "Load your dataset and split it into training and testing sets.\n",
        "Define the model and the hyperparameter grid for C, penalty, and solver.\n",
        "Use RandomizedSearchCV to search for the best hyperparameters.\n",
        "Fit the model and evaluate its accuracy.\n",
        "This approach allows you to efficiently find the optimal hyperparameters for your logistic regression model. For detailed implementation, you can refer to resources like the Scikit-learn documentation or tutorials on hyperparameter tuning. ```python import numpy as np import pandas as pd from sklearn.model_selection import train_test_split, RandomizedSearchCV from sklearn.linear_model import LogisticRegression from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from scipy.stats import uniform\n",
        "\n",
        "Load the Iris dataset\n",
        "iris = load_iris() X = iris.data y = iris.target\n",
        "\n",
        "Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "Scale the data using StandardScaler\n",
        "scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "Define the hyperparameter search space\n",
        "param_dist = { 'C': uniform(0.001, 100), # Range of C values 'penalty': ['l1', 'l2'], # Penalty types 'solver': ['liblinear', 'saga'] # Solvers }\n",
        "\n",
        "Create a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "\n",
        "Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(logistic_regression, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42) random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "Get the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "Fit the model with the best hyperparameters on the entire dataset\n",
        "best_model = random_search.best_estimator_ best_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "Evaluate the best model on the test set\n",
        "accuracy = best_model.score(X_test_scaled, y_test)\n",
        "\n",
        "Print the best hyperparameters and accuracy\n",
        "print(f\"Best Hyperparameters: {best_params}\") print(f\"Accuracy on test set: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OM5HUgNjYLO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Explanation of the Code\n",
        "\n",
        "- **Data Loading and Preprocessing**:\n",
        "  - The Iris dataset is loaded and split into training and testing sets.\n",
        "  - The features are scaled using `StandardScaler` to improve model performance.\n",
        "\n",
        "- **Hyperparameter Search Space**:\n",
        "  - A dictionary defines the hyperparameter distributions for `C`, `penalty`, and `solver`.\n",
        "\n",
        "- **Randomized Search**:\n",
        "  - `RandomizedSearchCV` is used to find the best hyperparameters by sampling from the defined distributions.\n",
        "\n",
        "- **Model Evaluation**:\n",
        "  - The best model is evaluated on the test set, and the best hyperparameters along with the accuracy are printed.\n",
        "\n",
        "This program effectively demonstrates how to use `RandomizedSearchCV` for hyperparameter tuning in logistic regression."
      ],
      "metadata": {
        "id": "1bDSTbE_YLUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy"
      ],
      "metadata": {
        "id": "C6p0udlhYLX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement One-vs-One (OvO) Multiclass Logistic Regression in Python, we can use the LogisticRegression class from the sklearn library along with the OneVsOneClassifier. Below is a complete example that demonstrates how to do this, including generating a synthetic dataset, training the model, and printing the accuracy.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "GTDS9Rj-YLcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN0RBYfRbVYn",
        "outputId": "70884d78-05fe-4eba-c64f-1b13dd6cc1af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a One-vs-One classifier with Logistic Regression\n",
        "ovo_classifier = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
        "\n",
        "# Train the classifier\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Accuracy of One-vs-One Multiclass Logistic Regression: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imvkfZA1bZmn",
        "outputId": "f3c607a5-da85-4c1b-eee2-00af8bd7b838"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of One-vs-One Multiclass Logistic Regression: 0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification"
      ],
      "metadata": {
        "id": "Eiu2COOXYLe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a Logistic Regression model and visualize the confusion matrix for binary classification in Python, you can use libraries such as scikit-learn for model training and evaluation, and matplotlib or seaborn for visualization. Below is a complete example that demonstrates this process, including generating a synthetic dataset, training the model, and visualizing the confusion matrix.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "kq_aeFjnYLhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn matplotlib seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDvSfKH6byyg",
        "outputId": "8f5833fe-13d7-4bb1-8ac3-b549bf294681"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Logistic Regression: {accuracy:.2f}')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "NxcruvP6bzBg",
        "outputId": "1fa4752c-564f-4ae1-c6d6-dadccb804213"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression: 0.84\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score"
      ],
      "metadata": {
        "id": "aBpBHS05YLkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score in Python, you can use the scikit-learn library. Below is a complete example that demonstrates this process, including generating a synthetic dataset, training the model, and calculating the evaluation metrics.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:\n",
        "\n"
      ],
      "metadata": {
        "id": "AEcIBdHaYLnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGv2AGLncDOr",
        "outputId": "b29a1dde-03ec-4d66-c970-7394e49a7aa6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Logistic Regression: {accuracy:.2f}')\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1-Score: {f1:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkgPDHQXcDRx",
        "outputId": "4346b7ae-fd33-4b3c-885a-19e9aec588e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression: 0.84\n",
            "Precision: 0.80\n",
            "Recall: 0.87\n",
            "F1-Score: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance"
      ],
      "metadata": {
        "id": "wwdl6xCcYLqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a Logistic Regression model on imbalanced data can lead to poor performance, especially for the minority class. To address this, you can apply class weights to the model, which helps the algorithm pay more attention to the minority class during training.\n",
        "\n",
        "Below is a complete Python program that demonstrates how to create an imbalanced dataset, train a Logistic Regression model with class weights, and evaluate its performance using accuracy, precision, recall, and F1-score.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "FCA-Pu1uYLtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J5RiXxTcXpI",
        "outputId": "fce363c0-e9d7-4d2d-c4d4-f525d2ab9c9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Generate an imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_informative=10, n_redundant=5,\n",
        "                           weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Logistic Regression: {accuracy:.2f}')\n",
        "\n",
        "# Print classification report (includes precision, recall, and F1-score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WI9pU5icXs2",
        "outputId": "68778258-3c5d-4e05-a904-4877b1d13d38"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression: 0.80\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.79      0.87       267\n",
            "           1       0.33      0.82      0.47        33\n",
            "\n",
            "    accuracy                           0.80       300\n",
            "   macro avg       0.65      0.81      0.67       300\n",
            "weighted avg       0.90      0.80      0.83       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance"
      ],
      "metadata": {
        "id": "6MHoTSFiYLvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a Logistic Regression model on the Titanic dataset, handle missing values, and evaluate its performance, you can follow these steps:\n",
        "\n",
        "Load the Titanic dataset.\n",
        "Handle missing values.\n",
        "Preprocess the data (e.g., encoding categorical variables).\n",
        "Split the data into training and testing sets.\n",
        "Train the Logistic Regression model.\n",
        "Evaluate the model's performance.\n",
        "Below is a complete Python program that demonstrates this process. For this example, we'll use the Titanic dataset from the seaborn library, which provides a convenient way to access the dataset.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "t36PtUFVYLyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GROE0kdZcnMS",
        "outputId": "3b177d5e-6ee9-428d-ef45-c0d7bab6ee73"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "titanic = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(titanic.head())\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
        "target = 'Survived'\n",
        "\n",
        "# Handle missing values in 'Age' and 'Embarked'\n",
        "titanic['Age'].fillna(titanic['Age'].median(), inplace=True)\n",
        "titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Define features and target\n",
        "X = titanic[features]\n",
        "y = titanic[target]\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "# Create transformers for numeric and categorical features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median'))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine transformers into a preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline that first transforms the data and then fits the model\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Logistic Regression: {accuracy:.2f}')\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yE9METtmcnPO",
        "outputId": "f8353315-e1a5-4c8e-8538-b7a9d9efcf59"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-413f5e58fd7e>:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['Age'].fillna(titanic['Age'].median(), inplace=True)\n",
            "<ipython-input-15-413f5e58fd7e>:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "A given column is not a column of the dataframe",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Embarked'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mcol_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Embarked'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-413f5e58fd7e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m             )\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    589\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             \u001b[0mtransformer_to_input_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A given column is not a column of the dataframe\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcolumn_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scalingM"
      ],
      "metadata": {
        "id": "7Zw-F_r_YL1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is an important preprocessing step in machine learning, especially for algorithms like Logistic Regression that are sensitive to the scale of the input features. Standardization (Z-score normalization) is one common method of feature scaling.\n",
        "\n",
        "In this example, we will:\n",
        "\n",
        "Generate a synthetic dataset.\n",
        "Train a Logistic Regression model without feature scaling.\n",
        "Train another Logistic Regression model with feature scaling (standardization).\n",
        "Compare the accuracy of both models.\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "51xzma3cYL4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3wu7n4TdDj1",
        "outputId": "c10763bf-0dbc-41bc-840c-bf7766396581"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Print accuracy without scaling\n",
        "print(f'Accuracy of Logistic Regression without scaling: {accuracy_no_scaling:.2f}')\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with scaling\n",
        "model_with_scaling = LogisticRegression(max_iter=1000)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Print accuracy with scaling\n",
        "print(f'Accuracy of Logistic Regression with scaling: {accuracy_with_scaling:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yrnswqUdDm-",
        "outputId": "753dff71-5241-426c-f321-308f01490bd3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression without scaling: 0.84\n",
            "Accuracy of Logistic Regression with scaling: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score"
      ],
      "metadata": {
        "id": "jn-40cqPYL65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a Logistic Regression model and evaluate its performance using the ROC-AUC score, you can follow these steps:\n",
        "\n",
        "Load a dataset (we'll use a synthetic dataset for this example).\n",
        "Split the dataset into training and testing sets.\n",
        "Train the Logistic Regression model.\n",
        "Make predictions and calculate the ROC-AUC score.\n",
        "The ROC-AUC score is a useful metric for evaluating the performance of a binary classification model, as it considers the trade-off between true positive rates and false positive rates.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "le5Ii_ILYL-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIbhTCK7dWeq",
        "outputId": "204f4933-675d-42b3-b9a0-c720c895fc77"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f'ROC-AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label='ROC Curve (area = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "Z8EI5qFXdWiA",
        "outputId": "a78b6f8f-7bdf-442e-83b0-14c0f9338e46"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.91\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracyM\n"
      ],
      "metadata": {
        "id": "QU87RwlxYMDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Logistic Regression, the parameter C is the inverse of the regularization strength. A smaller value of C indicates stronger regularization. To train a Logistic Regression model with a custom learning rate (in this case, C=0.5), you can use the LogisticRegression class from the scikit-learn library.\n",
        "\n",
        "Below is a complete Python program that demonstrates how to train a Logistic Regression model with C=0.5 and evaluate its accuracy using a synthetic dataset.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:\n",
        "\n"
      ],
      "metadata": {
        "id": "6HXv9gmYYMFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Logistic Regression with C=0.5: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FobwFv_3dxKj",
        "outputId": "8b7f1f30-0b25-4fe1-dfbf-b9f9e6eaaa03"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression with C=0.5: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score"
      ],
      "metadata": {
        "id": "QWYX0asFYMJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a Logistic Regression model and evaluate its performance using the ROC-AUC score, you can follow these steps:\n",
        "\n",
        "Load a dataset (we'll use a synthetic dataset for this example).\n",
        "Split the dataset into training and testing sets.\n",
        "Train the Logistic Regression model.\n",
        "Make predictions and calculate the ROC-AUC score.\n",
        "The ROC-AUC score is a useful metric for evaluating the performance of a binary classification model, as it considers the trade-off between true positive rates and false positive rates.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "PocSKAoZYMNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euMeZgqweVaY",
        "outputId": "30316606-11d5-4aa9-d1db-1957c5679878"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f'ROC-AUC Score: {roc_auc:.2f}')\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label='ROC Curve (area = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "3iUquMExeVdn",
        "outputId": "7f2425c2-e7ae-4f03-8b67-b1e872afd65e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.91\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficientsM"
      ],
      "metadata": {
        "id": "2l5F_NMoYMQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a Logistic Regression model and identify important features based on model coefficients, you can follow these steps:\n",
        "\n",
        "Load a dataset (we'll use a synthetic dataset for this example).\n",
        "Split the dataset into training and testing sets.\n",
        "Train the Logistic Regression model.\n",
        "Extract and display the model coefficients to identify important features.\n",
        "The coefficients of the Logistic Regression model indicate the importance of each feature. A higher absolute value of a coefficient means that the feature has a greater impact on the prediction.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "PI_NOkylYMT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQzdA4Hoeuwa",
        "outputId": "528eb500-484f-493f-fc95-fb3e6a1d50c5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Convert to DataFrame for better feature handling\n",
        "feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
        "X_df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame to display feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sort the features by the absolute value of the coefficients\n",
        "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Display the feature importance\n",
        "print(\"Feature Importance based on Model Coefficients:\")\n",
        "print(feature_importance[['Feature', 'Coefficient', 'Absolute Coefficient']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB1Dmr48exXz",
        "outputId": "a93ccd05-b5d9-4f2d-9a4a-0c2ab88808a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance based on Model Coefficients:\n",
            "       Feature  Coefficient  Absolute Coefficient\n",
            "19  Feature_19    -0.921139              0.921139\n",
            "11  Feature_11     0.728599              0.728599\n",
            "0    Feature_0    -0.705893              0.705893\n",
            "16  Feature_16     0.582855              0.582855\n",
            "13  Feature_13     0.534189              0.534189\n",
            "17  Feature_17    -0.474242              0.474242\n",
            "9    Feature_9     0.419264              0.419264\n",
            "5    Feature_5     0.384200              0.384200\n",
            "14  Feature_14    -0.349059              0.349059\n",
            "6    Feature_6    -0.317711              0.317711\n",
            "8    Feature_8    -0.310010              0.310010\n",
            "4    Feature_4     0.265225              0.265225\n",
            "1    Feature_1     0.254231              0.254231\n",
            "2    Feature_2     0.247390              0.247390\n",
            "3    Feature_3     0.223566              0.223566\n",
            "18  Feature_18    -0.138496              0.138496\n",
            "12  Feature_12     0.128225              0.128225\n",
            "15  Feature_15     0.109151              0.109151\n",
            "10  Feature_10     0.082707              0.082707\n",
            "7    Feature_7     0.054826              0.054826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "ScoreM"
      ],
      "metadata": {
        "id": "QITTKSP1YMXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cohen's Kappa is a statistical measure that is used to evaluate the agreement between two raters (or classifiers) who each classify items into mutually exclusive categories. In the context of machine learning, it can be used to assess the performance of a classification model by comparing the predicted labels with the true labels.\n",
        "\n",
        "To train a Logistic Regression model and evaluate its performance using Cohen's Kappa score, you can follow these steps:\n",
        "\n",
        "Load a dataset (we'll use a synthetic dataset for this example).\n",
        "Split the dataset into training and testing sets.\n",
        "Train the Logistic Regression model.\n",
        "Make predictions and calculate Cohen's Kappa score.\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "lR9LIMd6YMa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkMIZ0jXfAtK",
        "outputId": "f82470b5-0279-4d24-e306-07df0bf9d2de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Logistic Regression: {accuracy:.2f}')\n",
        "\n",
        "# Calculate Cohen's Kappa score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mK-t5grfAwn",
        "outputId": "0acd8347-d030-4a61-f2c5-7fc919cfba55"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression: 0.84\n",
            "Cohen's Kappa Score: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracyM"
      ],
      "metadata": {
        "id": "f57MbmLIYMdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a Logistic Regression model using different solvers (liblinear, saga, and lbfgs) and compare their accuracy, you can follow these steps:\n",
        "\n",
        "Load a dataset (we'll use a synthetic dataset for this example).\n",
        "Split the dataset into training and testing sets.\n",
        "Train the Logistic Regression model using each solver.\n",
        "Evaluate and compare the accuracy of each model.\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "YNeJG_FUYMgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCTBs6gcgBNW",
        "outputId": "287e9bb0-d31a-438a-9872-94da745a94d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of solvers to evaluate\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "# Train and evaluate Logistic Regression with different solvers\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = accuracy\n",
        "    print(f'Accuracy of Logistic Regression with solver \"{solver}\": {accuracy:.2f}')\n",
        "\n",
        "# Compare accuracies\n",
        "print(\"\\nComparison of Accuracies:\")\n",
        "for solver, accuracy in accuracies.items():\n",
        "    print(f'Solver: {solver}, Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1hqHU6egBQm",
        "outputId": "3c76bae3-69bf-43ec-b83b-2831816855ed"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression with solver \"liblinear\": 0.84\n",
            "Accuracy of Logistic Regression with solver \"saga\": 0.84\n",
            "Accuracy of Logistic Regression with solver \"lbfgs\": 0.84\n",
            "\n",
            "Comparison of Accuracies:\n",
            "Solver: liblinear, Accuracy: 0.84\n",
            "Solver: saga, Accuracy: 0.84\n",
            "Solver: lbfgs, Accuracy: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification"
      ],
      "metadata": {
        "id": "DEi75TyTYMja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a Logistic Regression model and visualize the Precision-Recall curve for binary classification, you can follow these steps:\n",
        "\n",
        "Load a dataset (we'll use a synthetic dataset for this example).\n",
        "Split the dataset into training and testing sets.\n",
        "Train the Logistic Regression model.\n",
        "Calculate precision and recall at various thresholds.\n",
        "Plot the Precision-Recall curve.\n",
        "The Precision-Recall curve is particularly useful for evaluating the performance of a binary classifier, especially when dealing with imbalanced datasets.\n",
        "\n",
        "Make sure you have the required libraries installed. You can install them using pip if you haven't already:"
      ],
      "metadata": {
        "id": "BZsJdMv4YMmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6k5qCzJgmDV",
        "outputId": "fccf2ff7-8b6b-4ab2-f8cf-8b6873818b41"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Calculate average precision score\n",
        "average_precision = average_precision_score(y_test, y_scores)\n",
        "print(f'Average Precision Score: {average_precision:.2f}')\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label='Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.axhline(0.5, color='red', linestyle='--')  # Reference line for precision\n",
        "plt.axvline(0.5, color='green', linestyle='--')  # Reference line for recall\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "cdxT5bKsgmGw",
        "outputId": "a8e03165-901f-4ad4-9d4d-16b5b6718793"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision Score: 0.86\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)M"
      ],
      "metadata": {
        "id": "uH8xpme3YMpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that demonstrates how to train a Logistic Regression model using the scikit-learn library and evaluate its performance using the Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "We'll use a synthetic dataset for this example, but you can replace it with your own dataset as needed.\n",
        "\n",
        "Python Program"
      ],
      "metadata": {
        "id": "BwbicNVCYMtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXk4iSX_g2GM",
        "outputId": "58fcaea8-1294-4296-ef09-603d908f6d12"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the MCC\n",
        "print(f'Matthews Correlation Coefficient: {mcc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM69APoWg2Jg",
        "outputId": "620e4474-7575-4eea-cd66-6cb4baab431a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient: 0.7172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling"
      ],
      "metadata": {
        "id": "-vOqp4B_YMw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Logistic Regression model on both raw and standardized data, and then compares their accuracy to see the impact of feature scaling.\n",
        "\n",
        "Python Program"
      ],
      "metadata": {
        "id": "R2Gb0FtbhM1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8pTRTRVhUQt",
        "outputId": "61a93541-c9fb-437f-ef0c-9202535d8829"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f'Accuracy on raw data: {accuracy_raw:.4f}')\n",
        "print(f'Accuracy on standardized data: {accuracy_scaled:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYhHdllChUV4",
        "outputId": "ed5d2511-3de8-4947-8274-24279f6abbae"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.8550\n",
            "Accuracy on standardized data: 0.8550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validationM"
      ],
      "metadata": {
        "id": "bn-L8UsNhM5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Logistic Regression model and finds the optimal regularization strength parameter ( C ) using cross-validation. We will use GridSearchCV from scikit-learn to perform cross-validation and hyperparameter tuning."
      ],
      "metadata": {
        "id": "tdC7e6gihM9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZc0M-YQhqd9",
        "outputId": "e0024875-7cac-4960-98b3-48920b91a1c1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the parameter grid for C\n",
        "param_grid = {\n",
        "    'C': np.logspace(-4, 4, 20)  # C values from 0.0001 to 10000\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best score\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Train the model with the best C on the entire training set\n",
        "best_model = LogisticRegression(C=best_C, max_iter=1000)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f'Optimal C: {best_C:.4f}')\n",
        "print(f'Best cross-validation accuracy: {best_score:.4f}')\n",
        "print(f'Test set accuracy with optimal C: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA34rLVXhqhh",
        "outputId": "28a479bc-e950-4cb7-e1f0-1ea435c1570e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 0.0886\n",
            "Best cross-validation accuracy: 0.8700\n",
            "Test set accuracy with optimal C: 0.8650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "4zk3YkpAhNET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a complete Python program that demonstrates how to train a Logistic Regression model, save the trained model using joblib, and then load it again to make predictions.\n",
        "\n",
        "Python Program"
      ],
      "metadata": {
        "id": "wWLiNX8dhNHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "905iXD-Uh-2X",
        "outputId": "666bb66f-0da7-4eb8-e780-5720ebd90570"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Save the trained model using joblib\n",
        "model_filename = 'logistic_regression_model.joblib'\n",
        "joblib.dump(model, model_filename)\n",
        "print(f'Model saved to {model_filename}')\n",
        "\n",
        "# Step 5: Load the model from the file\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print('Model loaded from file.')\n",
        "\n",
        "# Step 6: Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Step 7: Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Test set accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kCWXvA8h-6J",
        "outputId": "7d74788a-98b4-4c6a-e921-09de8ab09799"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to logistic_regression_model.joblib\n",
            "Model loaded from file.\n",
            "Test set accuracy: 0.8550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O798nwrWhNLF"
      }
    }
  ]
}