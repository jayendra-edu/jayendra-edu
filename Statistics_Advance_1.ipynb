{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeGsOVSuoEp9g+9lH9Niuf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayendra-edu/jayendra-edu/blob/main/Statistics_Advance_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the properties of the F-distribution."
      ],
      "metadata": {
        "id": "cR0GFnYNsnmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ans = The F-distribution, also known as Fisher's F-distribution, is a continuous probability distribution that arises in the context of hypothesis testing, especially in analysis of variance (ANOVA) and comparing variances of two populations. Here are the main properties of the F-distribution:\n",
        "\n",
        "Asymmetry and Skewness: The F-distribution is positively skewed, meaning it has a long right tail. As the degrees of freedom increase, the distribution becomes more symmetrical and approaches a normal distribution.\n",
        "\n",
        "Non-Negativity: The F-distribution takes only positive values because it is derived from the ratio of squared quantities (variances). This makes sense as variances and ratios of variances cannot be negative.\n",
        "\n",
        "Degrees of Freedom: The shape of the F-distribution depends on two degrees of freedom parameters:\n",
        "\n",
        "ùëë\n",
        "1\n",
        "d\n",
        "1\n",
        "‚Äã\n",
        " : the degrees of freedom of the numerator, representing the sample size of the first sample minus one.\n",
        "ùëë\n",
        "2\n",
        "d\n",
        "2\n",
        "‚Äã\n",
        " : the degrees of freedom of the denominator, representing the sample size of the second sample minus one.\n",
        "Mean of the F-Distribution: The mean exists only when ( d_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QPqDNMBxstdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests ?"
      ],
      "metadata": {
        "id": "E56H4t9quk8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is primarily used in the following types of statistical tests:\n",
        "\n",
        "ANOVA (Analysis of Variance):\n",
        "\n",
        "Purpose: ANOVA is used to compare the means of three or more groups to determine if at least one group mean is different from the others.\n",
        "Why F-distribution?: The F-distribution is appropriate here because it helps assess the ratio of variances between groups. If the group means are similar, the variance within groups will be small compared to the variance between groups, leading to a larger F-statistic.\n",
        "\n",
        "Regression Analysis:\n",
        "\n",
        "Purpose: In multiple regression, the F-test is used to determine if the overall model is significant, meaning that at least one of the predictors is related to the outcome variable.\n",
        "Why F-distribution?: The F-statistic compares the model's explained variance to the unexplained variance. A significant F-value indicates that the model explains a significant amount of the variability in the outcome.\n",
        "\n",
        "Comparing Variances:\n",
        "\n",
        "Purpose: The F-test can be used to compare the variances of two populations to see if they are significantly different.\n",
        "Why F-distribution?: The ratio of two sample variances follows an F-distribution, which allows us to test if the variances are equal."
      ],
      "metadata": {
        "id": "QvfBTAJcvFhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What are the key assumptions required for conducting an F-tests to compare the variance of two populations ?"
      ],
      "metadata": {
        "id": "-xn3TCESvXQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans = To conduct an F-test for comparing the variances of two populations, there are several key assumptions that need to be met:\n",
        "\n",
        "Independence: The two samples must be independent of each other. This means that the selection of one sample does not affect the selection of the other sample.\n",
        "\n",
        "Normality: The data in both populations should be approximately normally distributed. This assumption is particularly important for smaller sample sizes. If the sample sizes are large, the F-test is somewhat robust to deviations from normality.\n",
        "\n",
        "Homogeneity of Variances: The variances of the two populations should be equal (or at least similar). This is sometimes referred to as the assumption of \"equal variances.\" The F-test specifically tests this assumption.\n",
        "\n",
        "Summary:\n",
        "Independence: Samples must not influence each other.\n",
        "Normality: Data should be normally distributed.\n",
        "Equal Variances: The variances of the two populations should be similar."
      ],
      "metadata": {
        "id": "zB4i7Rnlv-cw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of ANOVA, and how does it differ from a t-test ?"
      ],
      "metadata": {
        "id": "cpXohHYCwN8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOVA (Analysis of Variance) is used to determine if there are statistically significant differences between the means of three or more groups. In contrast, a t-test is specifically designed to compare the means of only two groups.\n",
        "\n",
        "The key differences include:\n",
        "\n",
        "Number of Groups: AN OVA is used for three or more groups, while a t-test is used for two groups only.\n",
        "\n",
        "Hypothesis Testing:\n",
        "\n",
        "ANOVA tests the null hypothesis that all group means are equal.\n",
        "A t-test tests the null hypothesis that the means of two groups are equal.\n",
        "Type of Data:\n",
        "\n",
        "Both tests assume that the data is normally distributed and that the variances are equal (homogeneity of variance).\n",
        "ANOVA can handle more complex designs, such as factorial designs, while t-tests are limited to simpler comparisons.\n",
        "Error Rate:\n",
        "\n",
        "Using multiple t-tests increases the risk of Type I error (false positives) because each test carries its own error rate.\n",
        "ANOVA controls for this by testing all groups simultaneously, reducing the overall error rate.\n",
        "Post-hoc Analysis:\n",
        "\n",
        "If ANOVA indicates significant differences, post-hoc tests (like Tukey's HSD) are often conducted to determine which specific groups differ.\n",
        "A t-test does not require post-hoc analysis since it only compares two groups."
      ],
      "metadata": {
        "id": "6qOlKGV0xEmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain when and why you would use a one-way ANOVA Instead of multiple t-tests when comparing more than two groups?\n"
      ],
      "metadata": {
        "id": "vXaChlZuxn4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans = Using a one-way ANOVA instead of multiple t-tests is advisable when comparing more than two groups for several reasons:\n",
        "\n",
        "1. Control of Type I Error Rate:\n",
        "Type I Error: This refers to the probability of incorrectly rejecting the null hypothesis when it is actually true (a false positive). When you conduct multiple t-tests to compare three or more groups, the chance of encountering at least one Type I error increases with each additional test.\n",
        "One-way ANOVA: It allows you to test for differences among multiple groups simultaneously, thus controlling the overall Type I error rate. This means that you can make a single decision about the equality of means across all groups without inflating the risk of false positives.\n",
        "2. Efficiency:\n",
        "Performing multiple t-tests can be time-consuming and computationally inefficient, especially as the number of groups increases. One-way ANOVA condenses this process into a single analysis, making it quicker and more straightforward.\n",
        "3. Complexity of Comparisons:\n",
        "When comparing more than two groups, the relationships among the groups can be complex. ANOVA provides a framework to assess whether at least one group mean is different from the others without having to specify which groups are being compared in advance.\n",
        "4. Post-hoc Analysis:\n",
        "If the one-way ANOVA indicates significant differences among the groups, you can follow up with post-hoc tests (like Tukey's HSD, Bonferroni correction, etc.) to determine which specific groups differ from each other. This structured approach is more systematic than conducting multiple t-tests.\n",
        "5. Assumptions:\n",
        "Both t-tests and ANOVA assume normality and homogeneity of variance. However, ANOVA is designed to handle more complex designs and can be more robust to violations of these assumptions when proper methods (like transformations or non-parametric alternatives) are applied.\n",
        "Conclusion:"
      ],
      "metadata": {
        "id": "DnkK7NryxrbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain how variance is partitioned is ANOVA into between- group variance and within-group variance. How does this partitioning contribute to the calculation of the F- static?\n"
      ],
      "metadata": {
        "id": "RocUB05bx1wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ans = In ANOVA (Analysis of Variance), variance is partitioned into two main components: between-group variance and within-group variance. This partitioning is fundamental to understanding how ANOVA tests for differences among group means and contributes to the calculation of the F-statistic.\n",
        "\n",
        "1. Between-Group Variance:\n",
        "Definition: This component measures the variability in the group means relative to the overall mean of all groups. It reflects the differences between the means of the different groups.\n",
        "Calculation:\n",
        "First, calculate the overall mean of all data points.\n",
        "Then, for each group, compute the squared difference between the group mean and the overall mean, multiply by the number of observations in that group.\n",
        "The formula for between-group variance (often denoted as (SS_{between})) is: [ SS_{between} = \\sum_{i=1}^{k} n_i (\\bar{X}_i - \\bar{X})^2 ] where:\n",
        "(k) = number of groups\n",
        "(n_i) = number of observations in group (i)\n",
        "(\\bar{X}_i) = mean of group (i)\n",
        "(\\bar{X}) = overall mean of all groups.\n",
        "2. Within-Group Variance:\n",
        "Definition: This component measures the variability of observations within each group. It reflects how much the individual observations in each group differ from their respective group mean.\n",
        "Calculation:\n",
        "For each group, compute the squared differences between each observation and its group mean, and sum these squared differences across all groups.\n",
        "The formula for within-group variance (often denoted as (SS_{within})) is: [ SS_{within} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2 ] where:\n",
        "(X_{ij}) = individual observation (j) in group (i)\n",
        "(\\bar{X}_i) = mean of group (i).\n",
        "3. Total Variance:\n",
        "The total variance in the dataset can be expressed as: [ SS_{total} = SS_{between} + SS_{within} ]\n",
        "This total variance accounts for all the variability in the data.\n",
        "4. Calculation of the F-statistic:\n",
        "The F-statistic is the ratio of the mean square between groups (MSB) to the mean square within groups (MSW): [ F = \\frac{MSB}{MSW} ]\n",
        "Mean Squares:\n",
        "Mean Square Between (MSB): [ MSB = \\frac{SS_{between}}{k - 1} ] where (k - 1) is the degrees of freedom for between-group variance.\n",
        "Mean Square Within (MSW): [ MSW = \\frac{SS_{within}}{N - k} ] where (N - k) is the degrees of freedom for within-group variance, with (N) being the total number of observations.\n",
        "5. Interpretation:\n",
        "A larger F-statistic indicates a greater ratio of between-group variance to within-group variance, suggesting that the group means are significantly different from each other.\n",
        "If the null hypothesis (that all group means are equal) is true, the F-statistic will be close to 1, as the between-group variance will not be much larger than the within-group variance. A significantly larger F-statistic indicates that at least one group mean is different."
      ],
      "metadata": {
        "id": "Q1mLitqlypef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare the classical ( Frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertanity, parameter estimation, and hypothesis testing ?"
      ],
      "metadata": {
        "id": "tfL8nEvh1m3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here‚Äôs a straightforward comparison between the classical (Frequentist) approach to ANOVA and the Bayesian approach:\n",
        "\n",
        "1. Handling Uncertainty\n",
        "\n",
        "Frequentist ANOVA: Views uncertainty in terms of long-run frequencies. Here, parameters are fixed but unknown, and uncertainty comes from the data. Repeated sampling or experiments are thought to give similar results in the long run.\n",
        "Bayesian ANOVA: Treats parameters as random variables with probability distributions that reflect their uncertainty. Uncertainty is handled through \"prior distributions\" (beliefs about parameters before seeing data) and \"posterior distributions\" (updated beliefs after seeing data).\n",
        "\n",
        "2. Parameter Estimation\n",
        "\n",
        "Frequentist ANOVA: Estimates parameters, like group means, using point estimates, which are single best values (e.g., sample mean) without expressing uncertainty around them directly. Confidence intervals provide a range, but they rely on repeated sampling.\n",
        "Bayesian ANOVA: Produces a full probability distribution (posterior distribution) for each parameter, giving a richer description of uncertainty. It‚Äôs possible to find a \"credible interval\" (like a confidence interval, but interpreted as the range where the parameter likely lies based on observed data and prior belief).\n",
        "\n",
        "\n",
        "3. Hypothesis Testing ‚Åâ\n",
        "\n",
        "Frequentist ANOVA: Uses p-values to determine significance. Hypothesis testing is done by comparing the p-value to a threshold (usually 0.05). This means we're evaluating whether the observed effect would be likely under a \"null\" hypothesis (e.g., \"no difference between groups\") if the test were repeated many times.\n",
        "\n",
        "4. Bayesian ANOVA:\n",
        " Directly calculates the probability of hypotheses. For example, Bayesian ANOVA can assess the probability that group means are different by looking at the posterior distribution. It doesn‚Äôt rely on p-values and instead often reports the \"Bayes factor,\" which indicates the relative evidence for one hypothesis over another.\n",
        "\n",
        "Summary of Key Differences\n",
        "Aspect\tFrequentist ANOVA\tBayesian ANOVA\n",
        "Uncertainty\tBased on data frequency in long runs\tCaptured by prior and posterior beliefs\n",
        "Parameter Estimation\tPoint estimates and confidence intervals\tFull probability distributions (posterior)\n",
        "Hypothesis Testing\tUses p-values for significance\tUses posterior probability and Bayes factor"
      ],
      "metadata": {
        "id": "aAsvyynx1sf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Question: You have two sets of data representing the incomes of two different professions:\n",
        "\n",
        "Profession A: [48, 55, 60, 62, 59]\n",
        "Profession B: [45, 50, 55, 52, 47]\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "kCDJuwAi2DKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "1.     Python code to perform the F-test:\n",
        "\n"
      ],
      "metadata": {
        "id": "MFQFAfjx3cy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "data_A = [48, 55, 60, 62, 59]\n",
        "data_B = [45, 50, 55, 52, 47]\n",
        "f_statistic, p_value = stats.levene(data_A, data_B)  # Levene's test for equal variances\n",
        "print(\"F-statistic:\", f_statistic, \"P-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2Ecv7hZ3jSJ",
        "outputId": "44fca46e-1e8d-4646-d7de-8ebf17a91b7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 0.13793103448275856 P-value: 0.7199897873752998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation: If the p-value is below the significiance level(usually 0.05),it indicates that the variance are sinificantly different between the two professions."
      ],
      "metadata": {
        "id": "qRgjg-sC3_LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.\n",
        " Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:\n",
        "\n",
        "Region A: [160, 162, 165, 158, 164]\n",
        "Region B: [172, 175, 170, 168, 174]\n",
        "Region C: [182, 179, 185, 181, 183]\n",
        "Task: Write Python code to perform the one-way ANOVA and interpret the results.\n",
        "\n",
        "Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "31bHOXV_5rn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here‚Äôs the Python code you can use to conduct a one-way ANOVA on this data:\n",
        "\n"
      ],
      "metadata": {
        "id": "DAkQdtza55fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Heights data for each region\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [182, 179, 185, 181, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Output the results\n",
        "print(\"F-Statistic:\", f_statistic)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"There is a statistically significant difference in average heights between the regions.\")\n",
        "else:\n",
        "    print(\"There is no statistically significant difference in average heights between the regions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQT2ClM66Orp",
        "outputId": "330fde9c-5246-40d0-9b5e-057a704901b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-Statistic: 71.50467289719624\n",
            "P-Value: 2.1524852103763876e-07\n",
            "There is a statistically significant difference in average heights between the regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZwL0iR06geO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}