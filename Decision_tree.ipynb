{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1u4cB1I6fDCGzU8JG5Wrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayendra-edu/jayendra-edu/blob/main/Decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HU7YogLLhBca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a Decision Tree, and how does it work"
      ],
      "metadata": {
        "id": "8ajkZXMnhBeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a popular machine learning algorithm used for both classification and regression tasks. It is a tree-like model that makes decisions based on a series of questions or conditions about the input features. The structure of a Decision Tree consists of nodes, branches, and leaves:\n",
        "\n",
        "Nodes: Each internal node represents a feature (or attribute) of the dataset. It is where a decision is made based on the value of that feature.\n",
        "\n",
        "Branches: The branches represent the outcome of the decision made at the node. They lead to other nodes or to the final output.\n",
        "\n",
        "Leaves: The leaf nodes represent the final output or decision (class label in classification tasks or a continuous value in regression tasks).\n",
        "\n",
        "How Decision Trees Work:\n",
        "Splitting: The process begins at the root node, where the algorithm evaluates the features of the dataset to determine the best way to split the data. The goal is to create subsets of the data that are as homogeneous as possible with respect to the target variable. Various criteria can be used for splitting, such as:\n",
        "\n",
        "Gini Impurity: Measures the impurity of a dataset. Lower values indicate a more homogeneous dataset.\n",
        "Entropy: Measures the amount of disorder or uncertainty in the dataset. The goal is to minimize entropy after the split.\n",
        "Mean Squared Error (MSE): Used in regression tasks to minimize the variance of the target variable in the resulting subsets.\n",
        "Recursive Partitioning: The splitting process is applied recursively to each subset of the data, creating new nodes and branches until a stopping criterion is met. This could be a maximum depth of the tree, a minimum number of samples required to split a node, or when further splits do not significantly improve the model.\n",
        "\n",
        "Pruning: After the tree is built, it may be too complex and overfit the training data. Pruning is a technique used to remove sections of the tree that provide little power in predicting the target variable, thereby improving the model's generalization to unseen data.\n",
        "\n",
        "Prediction: To make predictions with a Decision Tree, you start at the root node and follow the branches based on the feature values of the input data until you reach a leaf node. The value or class label at the leaf node is the predicted output.\n",
        "\n",
        "Advantages of Decision Trees:\n",
        "Interpretability: Decision Trees are easy to understand and visualize, making them interpretable for non-experts.\n",
        "No Need for Feature Scaling: They do not require normalization or standardization of features.\n",
        "Handles Both Numerical and Categorical Data: Decision Trees can work with different types of data.\n",
        "Disadvantages of Decision Trees:\n",
        "Overfitting: They can easily overfit the training data, especially if the tree is deep.\n",
        "Instability: Small changes in the data can lead to different splits and a different tree structure.\n",
        "Bias towards Dominant Classes: In imbalanced datasets, Decision Trees can be biased towards the majority class."
      ],
      "metadata": {
        "id": "nDydl6DyhBiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are impurity measures in Decision Trees"
      ],
      "metadata": {
        "id": "SRLItDbohBjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In decision trees, impurity measures are metrics used to evaluate how well a particular split (or decision) separates the data into different classes. The goal of a decision tree is to create branches that lead to pure nodes, where each node contains instances of a single class. Impurity measures help in selecting the best feature and threshold for splitting the data at each node. Here are some common impurity measures used in decision trees:\n",
        "\n",
        "Gini Impurity:\n",
        "\n",
        "The Gini impurity measures the likelihood of a randomly chosen element being misclassified if it was randomly labeled according to the distribution of labels in the subset.\n",
        "It is calculated as: [ Gini(D) = 1 - \\sum_{i=1}^{C} p_i^2 ] where ( p_i ) is the proportion of instances belonging to class ( i ) in dataset ( D ), and ( C ) is the number of classes.\n",
        "A Gini impurity of 0 indicates a pure node.\n",
        "Entropy:\n",
        "\n",
        "Entropy is derived from information theory and measures the amount of disorder or uncertainty in the dataset.\n",
        "It is calculated as: [ Entropy(D) = -\\sum_{i=1}^{C} p_i \\log_2(p_i) ] where ( p_i ) is the proportion of instances belonging to class ( i ).\n",
        "Like Gini impurity, an entropy of 0 indicates a pure node.\n",
        "Classification Error:\n",
        "\n",
        "This measure is simpler and is defined as the proportion of misclassified instances in the dataset.\n",
        "It is calculated as: [ Error(D) = 1 - \\max(p_i) ] where ( \\max(p_i) ) is the maximum proportion of instances belonging to any single class.\n",
        "While easy to compute, it is less commonly used compared to Gini impurity and entropy.\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "For regression trees, the impurity measure can be the mean squared error, which quantifies the average of the squares of the errors (the difference between predicted and actual values).\n",
        "It is calculated as: [ MSE(D) = \\frac{1}{N} \\sum_{j=1}^{N} (y_j - \\bar{y})^2 ] where ( y_j ) is the actual value, ( \\bar{y} ) is the mean of the target values in the dataset, and ( N ) is the number of instances.\n",
        "Choosing an Impurity Measure\n",
        "The choice of impurity measure can affect the structure of the decision tree and its performance. Gini impurity and entropy often yield similar results, but they can lead to different splits in some cases. The choice may depend on the specific characteristics of the dataset and the goals of the analysis"
      ],
      "metadata": {
        "id": "LOQ6naJ-hBnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical formula for Gini Impurity"
      ],
      "metadata": {
        "id": "ni5irfxXhBpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gini impurity is a measure used to evaluate the impurity or purity of a dataset in the context of classification tasks, particularly in decision trees. The mathematical formula for Gini impurity is given by:\n",
        "\n",
        "[ Gini(D) = 1 - \\sum_{i=1}^{C} p_i^2 ]\n",
        "\n",
        "Where:\n",
        "\n",
        "( D ) is the dataset for which we are calculating the Gini impurity.\n",
        "( C ) is the number of classes in the dataset.\n",
        "( p_i ) is the proportion of instances in class ( i ) relative to the total number of instances in the dataset.\n",
        "Steps to Calculate Gini Impurity:\n",
        "Calculate the Proportions: For each class ( i ), calculate the proportion ( p_i ) of instances belonging to that class: [ p_i = \\frac{n_i}{N} ] where ( n_i ) is the number of instances in class ( i ) and ( N ) is the total number of instances in the dataset.\n",
        "\n",
        "Square the Proportions: Compute ( p_i^2 ) for each class.\n",
        "\n",
        "Sum the Squared Proportions: Sum all the squared proportions: [ \\sum_{i=1}^{C} p_i^2 ]\n",
        "\n",
        "Calculate Gini Impurity: Finally, apply the Gini formula: [ Gini(D) = 1 - \\sum_{i=1}^{C} p_i^2 ]\n",
        "\n",
        "Interpretation:\n",
        "A Gini impurity of 0 indicates that all instances belong to a single class (pure node).\n",
        "A Gini impurity closer to 1 indicates a higher level of impurity, meaning the instances are more evenly distributed among the classes.\n",
        "This measure is widely used in decision tree algorithms, such as the CART (Classification and Regression Trees) algorithm, to determine the best splits at each node.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ASUnMlCOhBt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is the mathematical formula for Entropy"
      ],
      "metadata": {
        "id": "MXPy8q7EhBvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy is a measure from information theory that quantifies the amount of uncertainty or disorder in a dataset. In the context of decision trees, it is used to evaluate the impurity of a dataset for classification tasks. The mathematical formula for entropy is given by:\n",
        "\n",
        "[ Entropy(D) = -\\sum_{i=1}^{C} p_i \\log_2(p_i) ]\n",
        "\n",
        "Where:\n",
        "\n",
        "( D ) is the dataset for which we are calculating the entropy.\n",
        "( C ) is the number of classes in the dataset.\n",
        "( p_i ) is the proportion of instances belonging to class ( i ) in the dataset.\n",
        "Steps to Calculate Entropy:\n",
        "Calculate the Proportions: For each class ( i ), calculate the proportion ( p_i ) of instances belonging to that class: [ p_i = \\frac{n_i}{N} ] where ( n_i ) is the number of instances in class ( i ) and ( N ) is the total number of instances in the dataset.\n",
        "\n",
        "Compute the Logarithm: For each class, compute ( \\log_2(p_i) ). Note that if ( p_i = 0 ), the term ( p_i \\log_2(p_i) ) is defined to be 0, as the limit of ( x \\log_2(x) ) as ( x ) approaches 0 is 0.\n",
        "\n",
        "Sum the Products: Multiply each ( p_i ) by its corresponding ( \\log_2(p_i) ) and sum these values: [ -\\sum_{i=1}^{C} p_i \\log_2(p_i) ]\n",
        "\n",
        "Calculate Entropy: The result from the previous step gives you the entropy of the dataset.\n",
        "\n",
        "Interpretation:\n",
        "An entropy of 0 indicates that all instances belong to a single class (pure node).\n",
        "Higher entropy values indicate greater disorder or uncertainty, with a maximum entropy occurring when the classes are evenly distributed.\n",
        "Entropy is commonly used in decision tree algorithms, such as the ID3 (Iterative Dichotomiser 3) algorithm, to determine the best splits at each node based on the reduction of uncertainty"
      ],
      "metadata": {
        "id": "nF7OPaU7hBz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is Information Gain, and how is it used in Decision Trees"
      ],
      "metadata": {
        "id": "1lSQZqKmhB1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain is a key concept in decision tree algorithms, particularly in the context of classification tasks. It measures the effectiveness of an attribute (or feature) in reducing uncertainty about the target variable (class label) when splitting the data. In simpler terms, it quantifies how much knowing the value of a feature improves our ability to predict the target variable.\n",
        "\n",
        "Definition of Information Gain\n",
        "Information Gain is calculated as the difference between the entropy of the original dataset and the weighted average of the entropies of the subsets created by splitting the dataset based on a particular feature. The formula for Information Gain ( IG ) when splitting a dataset ( D ) on an attribute ( A ) is given by:\n",
        "\n",
        "[ IG(D, A) = Entropy(D) - \\sum_{v \\in Values(A)} \\frac{|D_v|}{|D|} Entropy(D_v) ]\n",
        "\n",
        "Where:\n",
        "\n",
        "( IG(D, A) ) is the information gain from splitting dataset ( D ) on attribute ( A ).\n",
        "( Entropy(D) ) is the entropy of the original dataset.\n",
        "( Values(A) ) is the set of all possible values of attribute ( A ).\n",
        "( D_v ) is the subset of ( D ) for which attribute ( A ) has value ( v ).\n",
        "( |D_v| ) is the number of instances in subset ( D_v ).\n",
        "( |D| ) is the total number of instances in the dataset ( D ).\n",
        "Steps to Calculate Information Gain\n",
        "Calculate the Entropy of the Original Dataset: Compute the entropy of the dataset ( D ) using the entropy formula.\n",
        "\n",
        "Split the Dataset: For each value ( v ) of the attribute ( A ), create a subset ( D_v ) of the dataset.\n",
        "\n",
        "Calculate the Entropy of Each Subset: Compute the entropy for each subset ( D_v ).\n",
        "\n",
        "Compute the Weighted Average of the Entropies: Calculate the weighted average of the entropies of the subsets, where the weights are the proportions of instances in each subset relative to the original dataset.\n",
        "\n",
        "Calculate Information Gain: Subtract the weighted average entropy from the original entropy to get the information gain.\n",
        "\n",
        "Usage in Decision Trees\n",
        "Selecting the Best Attribute: In decision tree algorithms like ID3, the attribute with the highest information gain is chosen for the split at each node. This means that the algorithm prefers features that provide the most information about the target variable, effectively reducing uncertainty.\n",
        "\n",
        "Building the Tree: The process of calculating information gain is repeated recursively for each subset created by the split, allowing the tree to grow until a stopping criterion is met (e.g., all instances in a node belong to the same class, or a maximum tree depth is reached).\n",
        "\n",
        "Conclusion\n",
        "Information Gain is a fundamental concept in decision tree learning, guiding the construction of the tree by selecting the most informative features for splitting the data. By maximizing information gain at each step, decision trees aim to create a model that accurately classifies instances based on their features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GALe8jPXhB6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the difference between Gini Impurity and Entropy"
      ],
      "metadata": {
        "id": "XoQVGkOehB76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini impurity and entropy are both measures used to evaluate the impurity or purity of a dataset in the context of classification tasks, particularly in decision trees. While they serve a similar purpose, there are key differences between the two. Here’s a comparison of Gini impurity and entropy:\n",
        "\n",
        "1. Definition and Formula\n",
        "Gini Impurity:\n",
        "\n",
        "Gini impurity measures the probability of a randomly chosen element being misclassified if it was randomly labeled according to the distribution of labels in the dataset.\n",
        "The formula is: [ Gini(D) = 1 - \\sum_{i=1}^{C} p_i^2 ] where ( p_i ) is the proportion of instances belonging to class ( i ) in dataset ( D ), and ( C ) is the number of classes.\n",
        "Entropy:\n",
        "\n",
        "Entropy measures the amount of uncertainty or disorder in the dataset. It quantifies the expected amount of information needed to classify an instance.\n",
        "The formula is: [ Entropy(D) = -\\sum_{i=1}^{C} p_i \\log_2(p_i) ] where ( p_i ) is the proportion of instances belonging to class ( i ).\n",
        "2. Range of Values\n",
        "Gini Impurity:\n",
        "\n",
        "Gini impurity ranges from 0 to 0.5 for binary classification (0 to 1 for multi-class). A Gini impurity of 0 indicates a pure node (all instances belong to a single class).\n",
        "Entropy:\n",
        "\n",
        "Entropy ranges from 0 to ( \\log_2(C) ) for ( C ) classes. Like Gini impurity, an entropy of 0 indicates a pure node.\n",
        "3. Sensitivity to Class Distribution\n",
        "Gini Impurity:\n",
        "\n",
        "Gini impurity is more sensitive to the distribution of classes. It tends to favor larger classes and is less sensitive to the presence of smaller classes.\n",
        "Entropy:\n",
        "\n",
        "Entropy is more sensitive to the distribution of classes, especially when the classes are imbalanced. It can give more weight to smaller classes, which may lead to different splits compared to Gini impurity.\n",
        "4. Computational Complexity\n",
        "Gini Impurity:\n",
        "\n",
        "Gini impurity is generally faster to compute because it does not involve logarithmic calculations. It only requires squaring the proportions of classes.\n",
        "Entropy:\n",
        "\n",
        "Entropy involves logarithmic calculations, which can make it slightly more computationally intensive than Gini impurity.\n",
        "5. Usage in Decision Trees\n",
        "Gini Impurity:\n",
        "\n",
        "Gini impurity is used in the CART (Classification and Regression Trees) algorithm. It is often preferred in practice due to its computational efficiency and tendency to create balanced trees.\n",
        "Entropy:\n",
        "\n",
        "Entropy is used in algorithms like ID3 and C4.5. It is often preferred when the goal is to maximize information gain.\n",
        "Conclusion\n",
        "Both Gini impurity and entropy are effective measures for evaluating the quality of splits in decision trees. The choice between them may depend on the specific characteristics of the dataset, the algorithm being used, and the desired properties of the resulting decision tree. In many cases, they yield similar results, but there can be differences in the structure of the tree and the splits chosen based on the measure used.\n"
      ],
      "metadata": {
        "id": "O7BY8frXhCA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the mathematical explanation behind Decision Trees"
      ],
      "metadata": {
        "id": "Py1U3gdVhCCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are a popular machine learning algorithm used for classification and regression tasks. They are based on a tree-like model of decisions and their possible consequences. The mathematical explanation behind Decision Trees involves several key concepts, including entropy, information gain, Gini impurity, and the recursive partitioning of the feature space. Here’s a breakdown of these concepts:\n",
        "\n",
        "1. Structure of Decision Trees\n",
        "A Decision Tree consists of:\n",
        "\n",
        "Nodes: Represent features or attributes.\n",
        "Edges: Represent the decision rules.\n",
        "Leaves: Represent the outcome or class labels.\n",
        "2. Splitting Criteria\n",
        "To build a Decision Tree, we need to determine how to split the data at each node. The most common criteria for splitting are:\n",
        "\n",
        "a. Entropy and Information Gain\n",
        "Entropy: A measure of the impurity or disorder in a dataset. For a dataset ( S ) with ( C ) classes, the entropy ( H(S) ) is defined as: [ H(S) = -\\sum_{i=1}^{C} p_i \\log_2(p_i) ] where ( p_i ) is the proportion of instances in class ( i ).\n",
        "\n",
        "Information Gain: The reduction in entropy after a dataset is split on an attribute ( A ). It is calculated as: [ IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v) ] where ( S_v ) is the subset of ( S ) for which attribute ( A ) has value ( v ).\n",
        "\n",
        "b. Gini Impurity\n",
        "Another criterion for splitting is Gini impurity, which measures the impurity of a dataset. For a dataset ( S ): [ Gini(S) = 1 - \\sum_{i=1}^{C} p_i^2 ] where ( p_i ) is the proportion of instances in class ( i ). The Gini impurity is used in the CART (Classification and Regression Trees) algorithm.\n",
        "\n",
        "3. Recursive Partitioning\n",
        "The process of building a Decision Tree involves recursively splitting the dataset into subsets based on the chosen attribute that maximizes information gain or minimizes Gini impurity. This process continues until:\n",
        "\n",
        "A stopping criterion is met (e.g., maximum depth, minimum samples per leaf).\n",
        "All instances in a node belong to the same class (pure node).\n",
        "No further information gain can be achieved.\n",
        "4. Pruning\n",
        "To avoid overfitting, Decision Trees can be pruned. Pruning involves removing nodes that provide little predictive power. This can be done using techniques like cost complexity pruning, where a penalty is applied for the number of leaves in the tree.\n",
        "\n",
        "5. Mathematical Representation\n",
        "The Decision Tree can be represented mathematically as a function ( f(x) ) that maps input features ( x ) to output classes ( y ): [ f(x) = \\text{class}(x) \\text{ if } x \\text{ reaches leaf node} ]\n",
        "\n",
        "Conclusion\n",
        "In summary, Decision Trees use concepts from information theory (entropy, information gain) and probability (Gini impurity) to make decisions based on the features of the data. The recursive nature of the algorithm allows it to create a model that can classify or predict outcomes based on the learned decision rules.\n",
        "\n"
      ],
      "metadata": {
        "id": "t-mvUd6phCHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Pre-Pruning in Decision Trees"
      ],
      "metadata": {
        "id": "pZd3MTrThCIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-pruning, also known as early stopping, is a technique used in the construction of Decision Trees to prevent overfitting by halting the growth of the tree before it reaches its full depth. The idea is to stop the tree from splitting further when certain conditions are met, thereby simplifying the model and improving its generalization to unseen data.\n",
        "\n",
        "Key Concepts of Pre-Pruning\n",
        "Overfitting: This occurs when a model learns the noise in the training data rather than the underlying pattern. A very deep Decision Tree can perfectly classify the training data but may perform poorly on new, unseen data.\n",
        "\n",
        "Stopping Criteria: Pre-pruning involves setting specific criteria that determine when to stop splitting a node. Common criteria include:\n",
        "\n",
        "Maximum Depth: Limit the maximum depth of the tree. If a node reaches this depth, it will not be split further.\n",
        "Minimum Samples per Leaf: Specify a minimum number of samples that must be present in a leaf node. If a node has fewer samples than this threshold, it will not be split.\n",
        "Minimum Information Gain: Set a threshold for the minimum information gain required to make a split. If the gain from splitting a node is below this threshold, the node will not be split.\n",
        "Minimum Gini Impurity Reduction: Similar to information gain, this criterion requires a minimum reduction in Gini impurity for a split to be considered.\n",
        "Benefits of Pre-Pruning:\n",
        "\n",
        "Reduced Complexity: By stopping the growth of the tree early, pre-pruning helps create a simpler model that is easier to interpret.\n",
        "Improved Generalization: A less complex tree is less likely to overfit the training data, which can lead to better performance on unseen data.\n",
        "Faster Training: Pre-pruning can reduce the computational cost of training the model since fewer splits and nodes need to be evaluated.\n",
        "Trade-offs: While pre-pruning can help prevent overfitting, it can also lead to underfitting if the stopping criteria are too strict. This means that the model may not capture the underlying patterns in the data effectively. Therefore, it is essential to choose the stopping criteria carefully based on the specific dataset and problem.\n",
        "\n",
        "Example of Pre-Pruning\n",
        "Suppose you are building a Decision Tree to classify whether an email is spam or not. You might decide to implement pre-pruning by setting the following criteria:\n",
        "\n",
        "Maximum depth of the tree: 5\n",
        "Minimum samples per leaf: 10\n",
        "Minimum information gain: 0.01\n",
        "As you build the tree, if you reach a node that has a depth of 5, or if a potential split would result in a leaf with fewer than 10 samples, or if the information gain from the split is less than 0.01, you would stop splitting that node and make it a leaf.\n",
        "\n",
        "Conclusion\n",
        "Pre-pruning is a valuable technique in Decision Tree learning that helps to control the complexity of the model and improve its generalization capabilities. By setting appropriate stopping criteria, you can create a more robust model that performs well on both training and unseen data"
      ],
      "metadata": {
        "id": "DMSGnk4nhCNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is Post-Pruning in Decision Trees"
      ],
      "metadata": {
        "id": "yMSmScqZhCOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-pruning is a technique used in the construction of Decision Trees to reduce the complexity of the model after it has been fully grown. Unlike pre-pruning, which stops the growth of the tree during its construction, post-pruning involves removing nodes from a fully grown tree to improve its generalization performance and reduce overfitting.\n",
        "\n",
        "Key Concepts of Post-Pruning\n",
        "Overfitting: Similar to pre-pruning, post-pruning addresses the issue of overfitting, where a Decision Tree becomes too complex and captures noise in the training data rather than the underlying patterns.\n",
        "\n",
        "Full Tree Growth: In post-pruning, the Decision Tree is first allowed to grow to its maximum depth, creating a tree that perfectly classifies the training data (or nearly so). This tree may have many branches and leaves, which can lead to overfitting.\n",
        "\n",
        "Pruning Process: After the tree is fully grown, the pruning process begins. This involves evaluating the performance of the tree and deciding which nodes (or branches) can be removed without significantly affecting the model's accuracy. The steps typically include:\n",
        "\n",
        "Validation Set: A separate validation dataset is used to evaluate the performance of the tree. This helps in assessing how well the tree generalizes to unseen data.\n",
        "Node Evaluation: Each node is evaluated to determine whether removing it (and replacing it with its most common class label) would improve or maintain the accuracy of the tree on the validation set.\n",
        "Cost Complexity Pruning: A common method for post-pruning is cost complexity pruning, which involves balancing the tree's complexity (number of leaves) against its accuracy. A penalty is applied for the number of leaves, and nodes are pruned based on a cost-complexity parameter.\n",
        "Benefits of Post-Pruning:\n",
        "\n",
        "Improved Generalization: By removing unnecessary nodes, post-pruning helps to create a simpler model that is less likely to overfit the training data, leading to better performance on unseen data.\n",
        "Flexibility: Since the tree is fully grown before pruning, the model has the opportunity to capture complex patterns in the data, which can then be simplified afterward.\n",
        "Trade-offs: While post-pruning can effectively reduce overfitting, it may also lead to the loss of some useful information if important nodes are pruned away. Therefore, careful evaluation is necessary to ensure that the pruning process does not degrade the model's performance.\n",
        "\n",
        "Example of Post-Pruning\n",
        "Consider a Decision Tree that has been fully grown to classify whether a customer will buy a product based on various features (age, income, etc.). After the tree is constructed, you might:\n",
        "\n",
        "Use a validation dataset to evaluate the accuracy of the tree.\n",
        "Identify nodes that, when pruned, do not significantly decrease the accuracy on the validation set.\n",
        "Replace those nodes with their most common class label, effectively simplifying the tree.\n",
        "Conclusion\n",
        "Post-pruning is a valuable technique for refining Decision Trees after they have been fully constructed. By removing unnecessary complexity, post-pruning helps improve the model's generalization capabilities and reduces the risk of overfitting. It allows for a more robust model that can perform well on both training and unseen data, making it an essential step in the Decision Tree learning process.\n",
        "\n"
      ],
      "metadata": {
        "id": "7y1DTTkRhCTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the difference between Pre-Pruning and Post-Pruning"
      ],
      "metadata": {
        "id": "KU9YzvsUhCVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-pruning and post-pruning are two techniques used in the construction of Decision Trees to prevent overfitting and improve the model's generalization capabilities. However, they differ in their approach and timing in the tree-building process. Here are the key differences between the two:\n",
        "\n",
        "1. Timing of Pruning\n",
        "Pre-Pruning:\n",
        "\n",
        "Occurs during the construction of the Decision Tree.\n",
        "The growth of the tree is halted based on certain criteria before it reaches its full depth.\n",
        "Decisions to stop splitting nodes are made as the tree is being built.\n",
        "Post-Pruning:\n",
        "\n",
        "Occurs after the Decision Tree has been fully grown.\n",
        "The tree is allowed to grow to its maximum depth, capturing all possible patterns in the training data.\n",
        "Nodes are evaluated and removed after the tree is complete, based on their contribution to the model's performance.\n",
        "2. Methodology\n",
        "Pre-Pruning:\n",
        "\n",
        "Involves setting specific stopping criteria, such as:\n",
        "Maximum depth of the tree.\n",
        "Minimum number of samples required to split a node.\n",
        "Minimum information gain or Gini impurity reduction required for a split.\n",
        "The goal is to prevent the tree from becoming too complex from the outset.\n",
        "Post-Pruning:\n",
        "\n",
        "Involves evaluating the fully grown tree and determining which nodes can be removed without significantly affecting accuracy.\n",
        "Common methods include:\n",
        "Cost complexity pruning, which balances tree complexity against accuracy.\n",
        "Validation set evaluation to assess the impact of pruning on model performance.\n",
        "The goal is to simplify the tree after it has captured all relevant patterns.\n",
        "3. Impact on Model Complexity\n",
        "Pre-Pruning:\n",
        "\n",
        "May lead to a simpler model from the beginning, but it risks underfitting if the stopping criteria are too strict.\n",
        "The model may not capture all relevant patterns in the data.\n",
        "Post-Pruning:\n",
        "\n",
        "Allows for a more complex model initially, which can capture intricate patterns in the training data.\n",
        "The pruning process can effectively reduce complexity while retaining important information, leading to better generalization.\n",
        "4. Computational Efficiency\n",
        "Pre-Pruning:\n",
        "\n",
        "Can be more computationally efficient since it stops the growth of the tree early, reducing the number of nodes and splits evaluated.\n",
        "Post-Pruning:\n",
        "\n",
        "May require more computational resources since the tree is fully grown before pruning, necessitating additional evaluations to determine which nodes to remove.\n",
        "Summary\n",
        "| Feature | Pre-Pruning | Post-Pruning | |------------------------|--------------------------------------|--------------------------------------| | Timing | During tree construction | After tree is fully grown | | Methodology | Stops growth based on criteria | Evaluates and removes nodes | | Impact on Complexity | May lead to underfitting | Can capture complex patterns initially | | Computational Efficiency | More efficient due to early stopping | May require more resources |\n",
        "\n",
        "In summary, pre-pruning and post-pruning are complementary techniques aimed at improving the performance of Decision Trees, each with its own advantages and trade-offs. The choice between them depends on the specific dataset, problem, and desired balance between model complexity and generalization.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s0iHRTHChCZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is a Decision Tree Regressor"
      ],
      "metadata": {
        "id": "uMlpEywchCbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree Regressor is a type of machine learning model that uses a tree-like structure to make predictions for continuous target variables. Unlike a Decision Tree Classifier, which is used for classification tasks (where the output is a discrete label), a Decision Tree Regressor is specifically designed to predict numerical values.\n",
        "\n",
        "Key Features of Decision Tree Regressors\n",
        "Structure:\n",
        "\n",
        "The model consists of nodes, branches, and leaves, similar to a classification tree.\n",
        "Each internal node represents a decision based on the value of a feature, each branch represents the outcome of that decision, and each leaf node represents a predicted value (the output).\n",
        "Splitting Criteria:\n",
        "\n",
        "Decision Tree Regressors use different criteria for splitting nodes compared to classifiers. Common criteria include:\n",
        "Mean Squared Error (MSE): The average of the squared differences between the predicted values and the actual values. The goal is to minimize MSE when making splits.\n",
        "Mean Absolute Error (MAE): The average of the absolute differences between predicted values and actual values. This can also be used as a splitting criterion.\n",
        "Prediction:\n",
        "\n",
        "When making predictions, the input features are passed down the tree, following the decision rules until a leaf node is reached. The value associated with that leaf node is the predicted output for the input features.\n",
        "Handling Non-linearity:\n",
        "\n",
        "Decision Tree Regressors can capture non-linear relationships between features and the target variable without requiring any transformation of the input data.\n",
        "Interpretability:\n",
        "\n",
        "Decision Trees are easy to interpret and visualize. The decision-making process can be understood by following the path from the root to the leaf node.\n",
        "Advantages of Decision Tree Regressors\n",
        "Simplicity: They are easy to understand and interpret, making them suitable for exploratory data analysis.\n",
        "Non-linearity: They can model complex relationships without requiring linear assumptions.\n",
        "Feature Importance: Decision Trees can provide insights into the importance of different features in making predictions.\n",
        "Disadvantages of Decision Tree Regressors\n",
        "Overfitting: Decision Trees can easily overfit the training data, especially if they are allowed to grow too deep. This can lead to poor generalization on unseen data.\n",
        "Instability: Small changes in the data can lead to different tree structures, making them sensitive to noise.\n",
        "Bias: Decision Trees can be biased towards features with more levels (categories) if not properly managed.\n",
        "Regularization Techniques\n",
        "To mitigate overfitting, several techniques can be applied to Decision Tree Regressors:\n",
        "\n",
        "Pre-Pruning: Stopping the growth of the tree based on certain criteria (e.g., maximum depth, minimum samples per leaf).\n",
        "Post-Pruning: Allowing the tree to grow fully and then removing nodes that do not contribute significantly to the model's performance.\n",
        "Ensemble Methods: Techniques like Random Forests and Gradient Boosting can be used to combine multiple Decision Trees, improving robustness and accuracy.\n",
        "Conclusion\n",
        "A Decision Tree Regressor is a powerful and interpretable model for predicting continuous outcomes. While it has advantages in terms of simplicity and flexibility, care must be taken to manage overfitting and ensure that the model generalizes well to new data. By using regularization techniques and ensemble methods, the performance of Decision Tree Regressors can be significantly enhanced.\n"
      ],
      "metadata": {
        "id": "Xjpjv2jRhCgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are the advantages and disadvantages of Decision Trees"
      ],
      "metadata": {
        "id": "YlJET8p0hCiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are a widely used machine learning algorithm for both classification and regression tasks. They have several advantages and disadvantages that can influence their effectiveness in different scenarios. Here’s a detailed overview:\n",
        "\n",
        "Advantages of Decision Trees\n",
        "Interpretability:\n",
        "\n",
        "Decision Trees are easy to understand and interpret. The tree structure allows users to visualize the decision-making process, making it straightforward to explain predictions to non-technical stakeholders.\n",
        "Non-linearity:\n",
        "\n",
        "They can capture non-linear relationships between features and the target variable without requiring any transformation of the input data. This makes them versatile for various types of datasets.\n",
        "No Need for Feature Scaling:\n",
        "\n",
        "Decision Trees do not require normalization or standardization of features, as they are invariant to the scale of the input data.\n",
        "Handling of Mixed Data Types:\n",
        "\n",
        "They can handle both numerical and categorical data, making them suitable for a wide range of applications.\n",
        "Feature Importance:\n",
        "\n",
        "Decision Trees can provide insights into the importance of different features in making predictions, which can be useful for feature selection and understanding the underlying data.\n",
        "Robustness to Outliers:\n",
        "\n",
        "Decision Trees are relatively robust to outliers, as they make splits based on thresholds rather than fitting a global model.\n",
        "Minimal Data Preparation:\n",
        "\n",
        "They require less data preprocessing compared to other algorithms, such as linear regression or neural networks.\n",
        "Disadvantages of Decision Trees\n",
        "Overfitting:\n",
        "\n",
        "Decision Trees can easily overfit the training data, especially if they are allowed to grow too deep. This can lead to poor generalization on unseen data.\n",
        "Instability:\n",
        "\n",
        "Small changes in the data can lead to different tree structures, making Decision Trees sensitive to noise. This can result in high variance in predictions.\n",
        "Bias Towards Features with More Levels:\n",
        "\n",
        "Decision Trees can be biased towards features with more categories (levels) if not properly managed, which can affect the model's performance.\n",
        "Limited Expressiveness:\n",
        "\n",
        "While they can capture non-linear relationships, Decision Trees may struggle with very complex patterns compared to ensemble methods like Random Forests or Gradient Boosting.\n",
        "Greedy Algorithm:\n",
        "\n",
        "The splitting process is greedy, meaning it makes the best decision at each node without considering the global structure of the tree. This can lead to suboptimal trees.\n",
        "Difficulty in Modeling Interactions:\n",
        "\n",
        "Decision Trees may not effectively capture interactions between features unless explicitly modeled through additional splits.\n",
        "Conclusion\n",
        "Decision Trees are a powerful and interpretable tool for both classification and regression tasks, with several advantages that make them appealing for many applications. However, their susceptibility to overfitting, instability, and potential biases necessitate careful consideration and often the use of regularization techniques or ensemble methods to enhance their performance. Understanding these advantages and disadvantages is crucial for selecting the right model for a given problem.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IwyJaMdfhCuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  How does a Decision Tree handle missing values"
      ],
      "metadata": {
        "id": "NtGcm5PBhD2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees can handle missing values in several ways, depending on the implementation and the specific algorithm used. Here are some common strategies:\n",
        "\n",
        "Surrogate Splits: Some decision tree algorithms use surrogate splits, which are alternative splits that can be used when the primary split feature has a missing value. The algorithm identifies the best alternative feature that can be used to make a decision when the primary feature is not available.\n",
        "\n",
        "Imputation: Before building the decision tree, missing values can be imputed using various techniques, such as mean, median, mode, or more sophisticated methods like k-nearest neighbors (KNN) or regression imputation. This way, the dataset is complete before training the model.\n",
        "\n",
        "Separate Branch for Missing Values: Some implementations allow for a separate branch in the tree specifically for instances with missing values. This means that when a decision point reaches a feature with a missing value, it can follow a designated path that handles those cases.\n",
        "\n",
        "Weighted Splits: In some cases, the algorithm can assign weights to instances with missing values, allowing them to contribute to the decision-making process without being excluded entirely.\n",
        "\n",
        "Ignoring Missing Values: Some decision tree algorithms can simply ignore instances with missing values during the split calculations. This means that the algorithm will only consider the available data for making splits.\n",
        "\n",
        "Probabilistic Approaches: Some advanced methods may use probabilistic approaches to estimate the likelihood of different outcomes based on the available data, allowing the decision tree to make informed decisions even with missing values.\n",
        "\n",
        "The choice of method often depends on the specific implementation of the decision tree algorithm and the nature of the dataset. It's important to consider the implications of each approach on the model's performance and interpretability.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xjbDMtY5hD4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How does a Decision Tree handle categorical features"
      ],
      "metadata": {
        "id": "NNvxjazjhD80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees can handle categorical features effectively, and they do so in several ways. Here’s how they typically manage categorical data:\n",
        "\n",
        "Direct Splitting: Decision trees can directly use categorical features to create splits. For a categorical feature with multiple categories, the algorithm evaluates all possible splits based on the categories. For example, if a feature has three categories (A, B, C), the decision tree can create splits like:\n",
        "\n",
        "Split 1: If the feature is A, go left; otherwise, go right.\n",
        "Split 2: If the feature is B, go left; otherwise, go right.\n",
        "Split 3: If the feature is C, go left; otherwise, go right.\n",
        "Binary Encoding: In some cases, categorical features can be transformed into binary features using techniques like one-hot encoding. This involves creating a new binary feature for each category. For example, a categorical feature with three categories (A, B, C) would be transformed into three binary features:\n",
        "\n",
        "Is_A (1 if A, 0 otherwise)\n",
        "Is_B (1 if B, 0 otherwise)\n",
        "Is_C (1 if C, 0 otherwise)\n",
        "This allows the decision tree to treat each category as a separate feature.\n",
        "\n",
        "Ordinal Encoding: If the categorical feature has a natural order (ordinal), it can be encoded as integers. For example, a feature like \"Size\" with categories \"Small,\" \"Medium,\" and \"Large\" could be encoded as 1, 2, and 3, respectively. However, care must be taken with this approach, as it may imply a relationship that does not exist.\n",
        "\n",
        "Group Splitting: Some decision tree algorithms can group categories together based on their performance in predicting the target variable. This can help reduce the number of splits and improve the model's interpretability.\n",
        "\n",
        "Handling High Cardinality: For categorical features with a large number of categories (high cardinality), decision trees can still handle them, but it may lead to overfitting. Techniques like grouping infrequent categories into an \"Other\" category or using regularization methods can help mitigate this issue.\n",
        "\n",
        "Gini Impurity and Information Gain: When evaluating splits based on categorical features, decision trees use metrics like Gini impurity or information gain to determine the best split. These metrics assess how well a particular split separates the classes in the target variable.\n",
        "\n",
        "Overall, decision trees are quite flexible in handling categorical features, making them a popular choice for datasets with mixed data types. However, the choice of encoding and handling methods can significantly impact the model's performance and interpretability.\n"
      ],
      "metadata": {
        "id": "c_UzY6_ZhD-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  What are some real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "wENJ-UWfhEC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees are widely used in various fields due to their simplicity, interpretability, and effectiveness. Here are some real-world applications across different domains:\n",
        "\n",
        "1. Healthcare\n",
        "Disease Diagnosis: Decision trees can analyze patient symptoms and medical history to assist in diagnosing diseases. For example, they can help predict conditions like diabetes or heart disease based on various health indicators.\n",
        "Treatment Recommendations: They can suggest treatment plans based on patient characteristics and previous treatment outcomes, helping healthcare providers make informed decisions.\n",
        "2. Finance\n",
        "Credit Scoring: Financial institutions use decision trees to evaluate the creditworthiness of loan applicants by analyzing factors such as income, credit history, and existing debts.\n",
        "Fraud Detection: Decision trees can identify patterns indicative of fraudulent transactions by analyzing transaction data and customer behavior, helping to mitigate financial losses.\n",
        "3. Marketing\n",
        "Customer Segmentation: Businesses can use decision trees to segment customers based on purchasing behavior, demographics, and preferences, allowing for targeted marketing strategies.\n",
        "Churn Prediction: They can help predict customer churn by analyzing factors that contribute to customers leaving a service or product, enabling proactive retention strategies.\n",
        "4. Manufacturing\n",
        "Quality Control: Decision trees can identify factors that lead to defects in products, helping manufacturers improve quality control processes and reduce waste.\n",
        "Predictive Maintenance: They can predict equipment failures by analyzing operational data, enabling proactive maintenance and reducing downtime.\n",
        "5. Retail\n",
        "Inventory Management: Retailers can use decision trees to optimize inventory levels by predicting demand for products based on historical sales data and seasonal trends.\n",
        "Recommendation Systems: They can help build recommendation systems that suggest products to customers based on their past purchases and preferences.\n",
        "6. Telecommunications\n",
        "Customer Retention: Decision trees can analyze customer data to identify at-risk customers and develop strategies to retain them, improving customer loyalty.\n",
        "Network Optimization: They can be used to optimize network performance by analyzing usage patterns and identifying areas for improvement.\n",
        "7. Insurance\n",
        "Risk Assessment: Insurance companies use decision trees to evaluate the risk associated with insuring individuals or properties, helping to set premiums accordingly.\n",
        "Claims Processing: They can assist in automating claims processing by determining the likelihood of fraud or the validity of claims based on historical data.\n",
        "8. Environmental Science\n",
        "Ecological Modeling: Decision trees can model ecological data, helping to predict species distribution based on environmental factors.\n",
        "Pollution Prediction: They can help predict pollution levels based on various factors such as industrial activity, weather conditions, and geographical data.\n",
        "9. Education\n",
        "Student Performance Prediction: Decision trees can analyze student data to predict academic performance and identify students who may need additional support.\n",
        "Curriculum Development: They can help design curricula by analyzing the effectiveness of different teaching methods and materials.\n",
        "10. Human Resources\n",
        "Employee Attrition Prediction: Decision trees can be used to predict which employees are likely to leave the organization based on factors like job satisfaction, performance metrics, and demographic information.\n",
        "Recruitment: They can assist in evaluating job candidates by analyzing their qualifications and past experiences to determine the best fit for a position.\n",
        "These applications illustrate the versatility and effectiveness of decision trees in solving complex problems across various industries. Their ability to provide clear and interpretable results makes them a popular choice for many decision-making processes.\n",
        "\n"
      ],
      "metadata": {
        "id": "lNTEPe4PhEEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "kiUP7ROshEJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy"
      ],
      "metadata": {
        "id": "9bnw6EBwhELd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Decision Tree Classifier on the Iris dataset and prints the model's accuracy. The Iris dataset is a well-known dataset in machine learning that contains measurements of iris flowers from three different species.\n",
        "\n",
        "You can run this code in a Python environment where you have the necessary libraries installed, such as scikit-learn and pandas. If you don't have these libraries installed, you can install them using pip:"
      ],
      "metadata": {
        "id": "32bV6CZChEQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIbLmjpcl_0O",
        "outputId": "1eefc3ac-6dff-43ad-9436-09b00c915726"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwyUUHzNl_3Q",
        "outputId": "87327f8a-e801-4df4-ceef-0d5f2a0025c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the features importance."
      ],
      "metadata": {
        "id": "ZNxockc-hER9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Decision Tree Classifier using Gini impurity as the criterion and prints the feature importances. We'll use the Iris dataset for this example.\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn and pandas). If you haven't installed them yet, you can do so using pip:"
      ],
      "metadata": {
        "id": "uvddOSsvhEW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRLEJmojmime",
        "outputId": "b04941f1-5aaf-40a8-a24d-13c84971ce34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier using Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importances\n",
        "feature_names = iris.feature_names\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Uq5TWfmipc",
        "outputId": "1f97b5db-c044-478d-ae5f-c53808676440"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.906143\n",
            "3   petal width (cm)    0.077186\n",
            "1   sepal width (cm)    0.016670\n",
            "0  sepal length (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy"
      ],
      "metadata": {
        "id": "aQ9YRg5ChEY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Decision Tree Classifier using entropy as the splitting criterion and prints the model's accuracy. We'll use the Iris dataset for this example.\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn and pandas). If you haven't installed them yet, you can do so using pip:\n",
        "\n"
      ],
      "metadata": {
        "id": "OMj01XX5hEi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6fJUs96m0z_",
        "outputId": "f08c51d1-86fd-4774-ebf8-b917328732ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier using Entropy as the criterion\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU4qxk_am03e",
        "outputId": "4c556f45-6a46-4dfa-d70c-1cae6ffd2a9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Code:\n",
        "Import Libraries: The necessary libraries are imported, including load_iris to load the dataset, train_test_split to split the data, DecisionTreeClassifier to create the model, and accuracy_score to evaluate the model's performance.\n",
        "\n",
        "Load the Dataset: The Iris dataset is loaded, and the features (X) and target labels (y) are extracted.\n",
        "\n",
        "Split the Dataset: The dataset is split into training and testing sets, with 80% of the data used for training and 20% for testing.\n",
        "\n",
        "Create the Model: A Decision Tree Classifier is created using entropy as the splitting criterion.\n",
        "\n",
        "Train the Model: The classifier is trained on the training data.\n",
        "\n",
        "Make Predictions: The trained model is used to make predictions on the test set.\n",
        "\n",
        "Calculate Accuracy: The accuracy of the model is calculated by comparing the predicted labels with the actual labels from the test set.\n",
        "\n",
        "Print Accuracy: Finally, the accuracy is printed as a percentage.\n",
        "\n",
        "Running the Code\n",
        "You can copy and paste this code into a Python script or a Jupyter notebook and run it to see the model's accuracy on the Iris dataset. The accuracy may vary slightly each time you run it due to the random splitting of the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UEgYQ2AXhEk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)*"
      ],
      "metadata": {
        "id": "TgKvn7KkhEpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Decision Tree Regressor on a housing dataset and evaluates the model using Mean Squared Error (MSE). For this example, we'll use the California housing dataset, which is available in scikit-learn.\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn and pandas). If you haven't installed them yet, you can do so using pip:"
      ],
      "metadata": {
        "id": "l4T1uadxhEu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S2YNapHnROD",
        "outputId": "349fef72-47ac-47e4-c078-47cda654c490"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data  # Features\n",
        "y = housing.target  # Target variable (house prices)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the Mean Squared Error\n",
        "print(f'Mean Squared Error: {mse:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZE--iIznTMf",
        "outputId": "192a51b1-49ca-4449-f65d-8af011e2f9df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20 '***Write*** a Python program to train a Decision Tree Classifier and visualize the tree using graphviz*"
      ],
      "metadata": {
        "id": "ImqzW6AdhExG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize a Decision Tree Classifier using Graphviz in Python, you'll need to install a few libraries, including graphviz and pydotplus. You can install these libraries using pip:"
      ],
      "metadata": {
        "id": "Iq7is6nIhE1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install graphviz pydotplus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xecS3VDnmEL",
        "outputId": "446d6a98-09c1-42b9-fa49-ca75e7ba5a28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from pydotplus) (3.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, you may need to install Graphviz software on your system. You can download it from Graphviz's official website.\n",
        "\n",
        "Below is a Python program that trains a Decision Tree Classifier on the Iris dataset and visualizes the tree using Graphviz:"
      ],
      "metadata": {
        "id": "53RpoHjqhE3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the tree\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create a Graphviz source object\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_decision_tree\")  # Save the tree as a PDF file\n",
        "graph.view()  # Open the PDF file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TUudw_9JnmPw",
        "outputId": "87133df4-70ee-4e0b-d79e-8957abe6ca5f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'iris_decision_tree.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree*"
      ],
      "metadata": {
        "id": "MPePEsufhE8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains two Decision Tree Classifiers on the Iris dataset: one with a maximum depth of 3 and another that is fully grown (without any depth restriction). The program then compares their accuracies.\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn and pandas). If you haven't installed them yet, you can do so using pip:\n",
        "\n"
      ],
      "metadata": {
        "id": "vni8yVGFhE-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-BOKiIBoEfA",
        "outputId": "186e51a2-5f25-431e-febf-ad167c936735"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with a maximum depth of 3\n",
        "clf_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "# Train the classifier with max depth of 3\n",
        "clf_depth_3.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_depth_3 = clf_depth_3.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for the max depth of 3\n",
        "accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
        "\n",
        "# Create a fully grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the fully grown classifier\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for the fully grown tree\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f'Accuracy of Decision Tree with max depth of 3: {accuracy_depth_3 * 100:.2f}%')\n",
        "print(f'Accuracy of Fully Grown Decision Tree: {accuracy_full * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKLSfSGQoEpA",
        "outputId": "ec8731ba-3ac8-4d59-f7b4-6b49679ce453"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max depth of 3: 100.00%\n",
            "Accuracy of Fully Grown Decision Tree: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree*"
      ],
      "metadata": {
        "id": "m14OZ3UKhFCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains two Decision Tree Classifiers on the Iris dataset: one with a minimum samples split of 5 and another using the default settings. The program then compares their accuracies.\n",
        "\n"
      ],
      "metadata": {
        "id": "d4gve-ZthFEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpUYTi4poaQA",
        "outputId": "ebd706b1-80c3-47c7-a7f7-e539897c266c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_samples_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "\n",
        "# Train the classifier with min_samples_split=5\n",
        "clf_min_samples_split.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_min_samples_split = clf_min_samples_split.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for the min_samples_split=5\n",
        "accuracy_min_samples_split = accuracy_score(y_test, y_pred_min_samples_split)\n",
        "\n",
        "# Create a default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the default classifier\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for the default tree\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f'Accuracy of Decision Tree with min_samples_split=5: {accuracy_min_samples_split * 100:.2f}%')\n",
        "print(f'Accuracy of Default Decision Tree: {accuracy_default * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEXZW2U0oaTg",
        "outputId": "1981b0fe-2040-4bea-8684-5610f39e0fa3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with min_samples_split=5: 100.00%\n",
            "Accuracy of Default Decision Tree: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data*"
      ],
      "metadata": {
        "id": "YQFQdyP7hFJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is often important in machine learning, especially for algorithms that rely on distance metrics. However, decision trees are generally not sensitive to the scale of the features because they make splits based on the order of the data rather than the actual values. Nevertheless, for educational purposes, we can still demonstrate how to apply feature scaling and compare the accuracy of a Decision Tree Classifier trained on scaled versus unscaled data.\n",
        "\n",
        "In this example, we'll use the Iris dataset and apply Min-Max scaling to the features before training the model. We'll then compare the accuracy of the model trained on the scaled data with the model trained on the unscaled data.\n",
        "\n"
      ],
      "metadata": {
        "id": "HGa48b2KhFLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tE02oqwosaI",
        "outputId": "03e0f3f3-6f5c-4797-ec3f-8d99a53c99e3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier for unscaled data\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on unscaled data\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for unscaled data\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply Min-Max scaling to the features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Decision Tree Classifier for scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on scaled data\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy for scaled data\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f'Accuracy of Decision Tree on Unscaled Data: {accuracy_unscaled * 100:.2f}%')\n",
        "print(f'Accuracy of Decision Tree on Scaled Data: {accuracy_scaled * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R16v7HevoslH",
        "outputId": "7c6eae04-e7ed-4325-e40c-4948aafd6d0d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree on Unscaled Data: 100.00%\n",
            "Accuracy of Decision Tree on Scaled Data: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification*"
      ],
      "metadata": {
        "id": "fcvas9d0hFQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a Decision Tree Classifier using the One-vs-Rest (OvR) strategy for multiclass classification, you can use the OneVsRestClassifier from scikit-learn. This strategy involves training one classifier per class, where each classifier is trained to distinguish between one class and all other classes.\n",
        "\n",
        "Below is a Python program that demonstrates how to implement this using the Iris dataset:\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn and pandas). If you haven't installed them yet, you can do so using pip:"
      ],
      "metadata": {
        "id": "0V5aNZ2fhFSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeASSVpNo97Q",
        "outputId": "e5a54180-ccff-4b9f-8129-21812dd69f6b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create a One-vs-Rest Classifier\n",
        "ovr_classifier = OneVsRestClassifier(dt_classifier)\n",
        "\n",
        "# Train the One-vs-Rest Classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Accuracy of Decision Tree Classifier using One-vs-Rest strategy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6O9Glg2ypBtX",
        "outputId": "a5eaf5e1-c1ea-499e-c03f-23014c481463"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree Classifier using One-vs-Rest strategy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.  Write a Python program to train a Decision Tree Classifier and display the feature importance scores*"
      ],
      "metadata": {
        "id": "jeXFkx2JhFWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGFjH3mSpT6J",
        "outputId": "5310fcaa-79be-4356-b5c0-fc170d1d5c94"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importances\n",
        "feature_names = iris.feature_names\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(importance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVhjEGVjpUEZ",
        "outputId": "25d98ea7-11b2-40bc-d613-553e2d1daa7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Scores:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.906143\n",
            "3   petal width (cm)    0.077186\n",
            "1   sepal width (cm)    0.016670\n",
            "0  sepal length (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Decision Tree Classifier on the Iris dataset and displays the feature importance scores. Feature importance scores indicate how much each feature contributes to the model's predictions.\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn and pandas). If you haven't installed them yet, you can do so using pip:\n",
        "\n"
      ],
      "metadata": {
        "id": "YUMD0viVhFZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.  Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree"
      ],
      "metadata": {
        "id": "pu4i4BxDhFcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains two Decision Tree Regressors on the California housing dataset: one with a maximum depth of 5 and another that is fully grown (without any depth restriction). The program then compares their performance using Mean Squared Error (MSE) as the evaluation metric.\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn and pandas). If you haven't installed them yet, you can do so using pip:"
      ],
      "metadata": {
        "id": "gZvQdGAYhFej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buh3pcgtpmiY",
        "outputId": "40946bd8-e115-4cb0-daae-255235ac4f2d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data  # Features\n",
        "y = housing.target  # Target variable (house prices)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor with max_depth=5\n",
        "regressor_depth_5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "\n",
        "# Train the regressor with max_depth=5\n",
        "regressor_depth_5.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_depth_5 = regressor_depth_5.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for max_depth=5\n",
        "mse_depth_5 = mean_squared_error(y_test, y_pred_depth_5)\n",
        "\n",
        "# Create a fully grown Decision Tree Regressor\n",
        "regressor_full = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the fully grown regressor\n",
        "regressor_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_full = regressor_full.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the fully grown tree\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "# Print the Mean Squared Errors\n",
        "print(f'Mean Squared Error of Decision Tree with max_depth=5: {mse_depth_5:.2f}')\n",
        "print(f'Mean Squared Error of Fully Grown Decision Tree: {mse_full:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0Vviqbvpmoz",
        "outputId": "baebb0e9-ca7a-4d9e-dfcf-57385bfb29c9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Decision Tree with max_depth=5: 0.52\n",
            "Mean Squared Error of Fully Grown Decision Tree: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.  Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy*"
      ],
      "metadata": {
        "id": "GwRog3CrhFif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost Complexity Pruning (CCP) is a technique used to prune a decision tree by removing nodes that have little importance, which can help prevent overfitting. In scikit-learn, you can use the ccp_alpha parameter in the DecisionTreeClassifier to apply this pruning.\n",
        "\n",
        "Below is a Python program that trains a Decision Tree Classifier on the Iris dataset, applies Cost Complexity Pruning, and visualizes the effect on accuracy. We will evaluate the model's accuracy for different values of ccp_alpha.\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn, pandas, and matplotlib). If you haven't installed them yet, you can do so using pip:"
      ],
      "metadata": {
        "id": "BIA9g8jihFk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tie9APM3p7rA",
        "outputId": "6cc62131-9988-4a97-c05b-7dbc37d3f522"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get the effective alphas and the corresponding decision trees\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Create a list to store the accuracy for each alpha\n",
        "accuracies = []\n",
        "\n",
        "# Evaluate the model for each alpha\n",
        "for alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "    y_pred = clf_pruned.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Plot the accuracy vs. ccp_alpha\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, accuracies, marker='o', linestyle='-')\n",
        "plt.title('Effect of Cost Complexity Pruning on Accuracy')\n",
        "plt.xlabel('ccp_alpha')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(np.arange(0, max(ccp_alphas) + 0.01, 0.01))\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print the best alpha and corresponding accuracy\n",
        "best_alpha = ccp_alphas[np.argmax(accuracies)]\n",
        "best_accuracy = max(accuracies)\n",
        "print(f'Best ccp_alpha: {best_alpha:.4f}, Best Accuracy: {best_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "Ufmwwa0Cp7uh",
        "outputId": "8a013238-e30e-4489-b14b-6f21800fce65"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best ccp_alpha: 0.0000, Best Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.  Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score*\n"
      ],
      "metadata": {
        "id": "bjVgts9ShFov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Decision Tree Classifier on the Iris dataset and evaluates its performance using Precision, Recall, and F1-Score. These metrics are particularly useful for understanding the performance of a classification model, especially in cases where the class distribution is imbalanced.\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn and pandas). If you haven't installed them yet, you can do so using pip:"
      ],
      "metadata": {
        "id": "3pbzb3FMhFrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQHmoTQvqQS4",
        "outputId": "3115eccd-192a-4f63-904d-54820b2930ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1-Score: {f1:.2f}')\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef-AdIkCqQW1",
        "outputId": "9b384385-6a9e-4dbb-832a-cf0e04eab1b9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29 Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn*"
      ],
      "metadata": {
        "id": "BdD10OIMhFvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Decision Tree Classifier on the Iris dataset and visualizes the confusion matrix using the seaborn library. The confusion matrix provides a summary of the prediction results on a classification problem, showing the counts of true positive, true negative, false positive, and false negative predictions."
      ],
      "metadata": {
        "id": "-VAeGvQChF1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas seaborn matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0THW6DoqjE3",
        "outputId": "cf525064-ff39-45a9-cf7f-9d9d45f1746f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "cm_df = pd.DataFrame(cm, index=iris.target_names, columns=iris.target_names)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False, square=True)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "jRLWz6W3qiIp",
        "outputId": "501ac4df-04d0-4095-a33e-545a61adf771"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split."
      ],
      "metadata": {
        "id": "pcEAnuHrhF4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Below is a Python program that trains a Decision Tree Classifier on the Iris dataset and uses GridSearchCV to find the optimal values for max_depth and min_samples_split. This process involves performing an exhaustive search over specified parameter values for the classifier.\n",
        "\n",
        "Make sure you have the necessary libraries installed (scikit-learn and pandas). If you haven't installed them yet, you can do so using pip:"
      ],
      "metadata": {
        "id": "ufyFrXPkhF_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRpwWQL2rGgi",
        "outputId": "84c345ed-1500-4d3d-a17b-e203f1d92ef4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for max_depth and min_samples_split\n",
        "param_grid = {\n",
        "    'max_depth': [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Make predictions using the best estimator\n",
        "best_clf = grid_search.best_estimator_\n",
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Best Cross-Validation Score: {best_score:.2f}')\n",
        "print(f'Test Set Accuracy: {test_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RBGZk8arGl4",
        "outputId": "48deb97c-521c-4b9f-97ef-16aaf30ab8e1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Best Cross-Validation Score: 0.94\n",
            "Test Set Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hBv3VbNshGBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "15o3_XqghGFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0ldCDt03hGJv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x347dEtWrnP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PfPaYLa2hGMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E6ZhyW7RhGQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7KvDyfOehGXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gnzqjb3vhGaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jaLlSFd0hGeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v6hxQDeThGg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LJumWDLhhGkv"
      }
    }
  ]
}